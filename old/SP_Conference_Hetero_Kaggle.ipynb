{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "s2qSNYEZJvXo"
      },
      "outputs": [],
      "source": [
        "# #@title PyG Installation { form-width: \"25%\" }\n",
        "# # enter these commands in CLI to install Pytorch-Geometric\n",
        "# !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
        "# !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
        "# !pip install -q git+https://github.com/rusty1s/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "qQ7zuNoJKXec"
      },
      "outputs": [],
      "source": [
        "#@title Module Imports { form-width: \"20%\" }\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.nn import Module,\\\n",
        "                     ModuleList,\\\n",
        "                     Embedding,\\\n",
        "                     BatchNorm1d,\\\n",
        "                     LogSoftmax,\\\n",
        "                     Softmax,\\\n",
        "                     Linear,\\\n",
        "                     NLLLoss,\\\n",
        "                     CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric as PyG\n",
        "from torch_geometric.data import Data, HeteroData\n",
        "from torch_geometric.nn.conv import RGCNConv, GINConv, GATConv, HeteroConv, GCNConv\n",
        "from torch_geometric.utils import to_networkx\n",
        "from collections import OrderedDict as od\n",
        "import logging\n",
        "import json\n",
        "from typing import NoReturn\n",
        "import typing\n",
        "from new.Utils import Globals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnJRJ5pTW_rR",
        "outputId": "151d19d6-27ec-469a-cc32-69e4b4dcc558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device is cuda\n"
          ]
        }
      ],
      "source": [
        "#@title Global Variables\n",
        "# Global Values\n",
        "\n",
        "print(f'Device is {Globals.DEVICE.value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "mH-QslHEJdRg"
      },
      "outputs": [],
      "source": [
        "#@title GNN Model { form-width: \"10%\" }\n",
        "class GNN(Module):\n",
        "  def __init__(self, embedding_dims: tuple, conv_dims: list, fully_connected_dims: list, dropout: dict)-> NoReturn:\n",
        "    super(GNN, self).__init__()\n",
        "\n",
        "    self.mode = None # 'train' or 'test' or 'dev' later \n",
        "    self.output_dim = 3 #home_result: win, lose, tie\n",
        "    self.num_relations = 7 #win/lose/tie/play/use/after/before\n",
        "    self.dropout = dropout\n",
        "\n",
        "    #one-hot to latent\n",
        "    self.embed = Embedding(embedding_dims[0], embedding_dims[1])\n",
        "\n",
        "    conv_list = [\n",
        "                  RGCNConv(embedding_dims[1], conv_dims[0], self.num_relations)\n",
        "                ] + \\\n",
        "                [\n",
        "                  RGCNConv(conv_dims[i], conv_dims[i+1], self.num_relations)\n",
        "                  for i in range(len(conv_dims[:-1]))\n",
        "                ]\n",
        "  \n",
        "    batch_norm_list = [\n",
        "                         BatchNorm1d(conv_dims[i])\n",
        "                         for i in range(len(conv_dims[:-1]))\n",
        "                      ]\n",
        "\n",
        "    fully_connected_list =   [\n",
        "                                Linear(2*conv_dims[-1], fully_connected_dims[0])\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(fully_connected_dims[i], fully_connected_dims[i+1])\n",
        "                                for i in range(len(fully_connected_dims[:-1]))\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(fully_connected_dims[-1], self.output_dim)\n",
        "                             ]\n",
        "    #graph conv layers\n",
        "    self.conv_layers = ModuleList(conv_list)\n",
        "    #batch normalization layers\n",
        "    self.batch_norm_layers = ModuleList(batch_norm_list)\n",
        "    #fully connected dense layers\n",
        "    self.fully_connected_layers = ModuleList(fully_connected_list)\n",
        "\n",
        "    self.classifier = LogSoftmax()\n",
        "\n",
        "    \n",
        "  def reset_parameters(self):\n",
        "        for conv in self.conv_layers:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.batch_norm_layers:\n",
        "            bn.reset_parameters()\n",
        "        for fc in self.fully_connected_layers:\n",
        "            fc.reset_parameters()\n",
        "          \n",
        "\n",
        "  def forward(self, x:torch.Tensor, edge_index:torch.Tensor, edge_type:torch.Tensor, home_list:list, away_list:list) -> torch.Tensor:\n",
        "    x = self.embed(x)\n",
        "    if self.training:\n",
        "      x = F.dropout(x, p=self.dropout[\"emb\"])\n",
        "\n",
        "    for conv, bn in zip(self.conv_layers[:-1], self.batch_norm_layers):\n",
        "      x = conv(x, edge_index=edge_index, edge_type=edge_type)\n",
        "      x = bn(x)\n",
        "      x = F.relu(x)\n",
        "      if self.training:\n",
        "        x = F.dropout(x, p=self.dropout[\"conv\"])\n",
        "\n",
        "\n",
        "    x = self.conv_layers[-1](x, edge_index, edge_type)\n",
        "    if self.training:\n",
        "      x = F.dropout(x, p=self.dropout[\"conv\"])\n",
        "\n",
        "    ##################################### End of Encoder \n",
        "\n",
        "    pred = list()\n",
        "    for home_team, away_team in zip(home_list, away_list):\n",
        "      h = torch.cat((x[home_team], x[away_team]))\n",
        "\n",
        "      for fc in self.fully_connected_layers[:-1]:\n",
        "        h = fc(h)\n",
        "        h = F.relu(h)\n",
        "        if self.training:\n",
        "          h = F.dropout(h, p=self.dropout[\"fc\"])\n",
        "\n",
        "      h = self.fully_connected_layers[-1](h)\n",
        "      if self.training:\n",
        "        h = F.dropout(h, p=self.dropout[\"fc\"])\n",
        "      pred.append(self.classifier(h))\n",
        "\n",
        "    return torch.stack(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "dV9ja7WmvjSW"
      },
      "outputs": [],
      "source": [
        "#@title HeteroGNN Model { form-width: \"10%\" }\n",
        "class HeteroGNN(Module):\n",
        "  def __init__(self, embedding_dims: tuple, conv_dims: list, fully_connected_dims: list, dropout: dict)-> NoReturn:\n",
        "    super(HeteroGNN, self).__init__()\n",
        "\n",
        "    self.mode = None # 'train' or 'test' or 'dev' later \n",
        "    self.output_dim = 3 #home_result: win, lose, tie\n",
        "    self.num_relations = 7 #win/lose/tie/play/use/after/before\n",
        "    self.dropout = dropout\n",
        "\n",
        "    #one-hot to latent\n",
        "    self.embed = Embedding(embedding_dims[0], embedding_dims[1])\n",
        "    \n",
        "    conv_list = [\n",
        "                  HeteroConv(\n",
        "                      {\n",
        "                          ('team', 'won', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'lost_to', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'tied_with', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('player', 'played_for', 'team'): GATConv(embedding_dims[-1], conv_dims[0], heads=1),\n",
        "                          ('team', 'used', 'player'): GATConv(embedding_dims[-1], conv_dims[0], heads=1),\n",
        "                          ('player', 'is_before', 'player'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('player', 'is_after', 'player'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'is_before', 'team'): GCNConv(embedding_dims[-1], conv_dims[0]),\n",
        "                          ('team', 'is_after', 'team'): GCNConv(embedding_dims[-1], conv_dims[0])\n",
        "                      }, aggr='sum'\n",
        "                  )\n",
        "                ] + \\\n",
        "                [\n",
        "                  HeteroConv(\n",
        "                      {\n",
        "                          ('team', 'won', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'lost_to', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'tied_with', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('player', 'played_for', 'team'): GATConv(conv_dims[i], conv_dims[i+1], heads=1),\n",
        "                          ('team', 'used', 'player'): GATConv(conv_dims[i], conv_dims[i+1], heads=1),\n",
        "                          ('player', 'is_before', 'player'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('player', 'is_after', 'player'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'is_before', 'team'): GCNConv(conv_dims[i], conv_dims[i+1]),\n",
        "                          ('team', 'is_after', 'team'): GCNConv(conv_dims[i], conv_dims[i+1])\n",
        "                      }, aggr='sum'\n",
        "                  )\n",
        "                  for i in range(len(conv_dims[:-1]))\n",
        "                ]\n",
        "\n",
        "\n",
        "              \n",
        "\n",
        "  \n",
        "    # batch_norm_list = [\n",
        "    #                      BatchNorm1d(conv_dims[i])\n",
        "    #                      for i in range(len(conv_dims[:-1]))\n",
        "    #                   ]\n",
        "\n",
        "    fully_connected_list =   [\n",
        "                                Linear(2*conv_dims[-1], fully_connected_dims[0])\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(fully_connected_dims[i], fully_connected_dims[i+1])\n",
        "                                for i in range(len(fully_connected_dims[:-1]))\n",
        "                             ] + \\\n",
        "                             [\n",
        "                                Linear(fully_connected_dims[-1], self.output_dim)\n",
        "                             ]\n",
        "    #graph conv layers\n",
        "    self.conv_layers = ModuleList(conv_list)\n",
        "    #batch normalization layers\n",
        "\n",
        "    # self.batch_norm_layers = ModuleList(batch_norm_list)\n",
        "\n",
        "    #fully connected dense layers\n",
        "    self.fully_connected_layers = ModuleList(fully_connected_list)\n",
        "\n",
        "    self.classifier = LogSoftmax(dim=1)\n",
        "      \n",
        "\n",
        "  def reset_parameters(self):\n",
        "      self.embed.reset_parameters()\n",
        "      for conv in self.conv_layers:\n",
        "          # for layer in conv:\n",
        "          #   layer.reset_parameters()\n",
        "          conv.reset_parameters()\n",
        "      # for bn in self.batch_norm_layers:\n",
        "      #     bn.reset_parameters()\n",
        "      for fc in self.fully_connected_layers:\n",
        "          fc.reset_parameters()\n",
        "\n",
        "\n",
        "  def forward(self, data: HeteroData) -> torch.Tensor:\n",
        "    x_dict = data.x_dict\n",
        "    home_list = data.home_list\n",
        "    away_list = data.away_list\n",
        "\n",
        "    edge_index_dict = data.edge_index_dict\n",
        "    x_dict = {key: self.embed(x) for key, x in x_dict.items()}\n",
        "    \n",
        "    if self.training:\n",
        "      x_dict = {key: F.dropout(x, p=self.dropout[\"emb\"]) for key, x in x_dict.items()}\n",
        "\n",
        "    # for conv, bn in zip(self.conv_layers[:-1], self.batch_norm_layers):\n",
        "    for conv in self.conv_layers[:-1]:\n",
        "      x_dict = conv(x_dict, edge_index_dict=edge_index_dict)\n",
        "      x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
        "      if self.training:\n",
        "        x_dict = {key: F.dropout(x, p=self.dropout[\"conv\"]) for key, x in x_dict.items()}\n",
        "\n",
        "    x_dict = self.conv_layers[-1](x_dict, edge_index_dict=edge_index_dict)\n",
        "    if self.training:\n",
        "      x_dict = {key: F.dropout(x, p=self.dropout[\"conv\"]) for key, x in x_dict.items()}\n",
        "\n",
        "    ##################################### End of Encoder \n",
        "    h = torch.cat(\n",
        "        (x_dict['team'][home_list], x_dict['team'][away_list]),\n",
        "        dim=1\n",
        "    )\n",
        "\n",
        "    for fc in self.fully_connected_layers[:-1]:\n",
        "      h = fc(h)\n",
        "      h = F.relu(h)\n",
        "      if self.training:\n",
        "        h = F.dropout(h, p=self.dropout[\"fc\"])\n",
        "\n",
        "    h = self.fully_connected_layers[-1](h)\n",
        "    if self.training:\n",
        "      h = F.dropout(h, p=self.dropout[\"fc\"])\n",
        "\n",
        "    return self.classifier(h)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "bFjPaR3fVEju"
      },
      "outputs": [],
      "source": [
        "#@title home_result(row)\n",
        "def home_result(row: str) -> int:\n",
        "  if row == 'home':\n",
        "    return Globals.WON.value\n",
        "  elif row == 'tie':\n",
        "    return Globals.TIED_WITH.value\n",
        "  elif row == 'away':\n",
        "    return Globals.LOST_TO.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "8bDOBoy9YJdW"
      },
      "outputs": [],
      "source": [
        "#@title remove_redundancy(players) { form-width: \"15%\" }\n",
        "def remove_redundancy(players: list) -> list:\n",
        "  new_players = list()\n",
        "\n",
        "  for player in players:\n",
        "    if 'Own' in player:\n",
        "      player = player.replace('Own', '')\n",
        "    if 'Pen. Scored' in player:\n",
        "      player = player.replace('Pen. Scored', '')\n",
        "    if 'Pen. Score' in player:\n",
        "      player = player.replace('Pen. Score', '')\n",
        "    if 'Own' in player or 'Scored' in player or 'Score' in player:\n",
        "      print(player)\n",
        "      #SHOULD NOT PRINT IF CODE IS CORRECT\n",
        "    else:\n",
        "      new_players.append(player.strip())\n",
        "  return new_players"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "76-OzUe-8V6_"
      },
      "outputs": [],
      "source": [
        "#@title extract_players(home_lineup, away_lineup) { form-width: \"15%\" }\n",
        "def extract_players(home_lineup: str, away_lineup: str) -> list:\n",
        "  home_players = home_lineup[:-2].split(' - ')\n",
        "  away_players = away_lineup[:-2].split(' - ')\n",
        "  \n",
        "  return remove_redundancy(home_players), remove_redundancy(away_players)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "fdvbloHFNou-"
      },
      "outputs": [],
      "source": [
        "#@title stats(df, show_players, show_teams, show_results) { form-width: \"10%\" }\n",
        "def stats(df: pd.DataFrame, show_players: bool=False, show_teams: bool=False, show_results: bool=False) -> NoReturn:\n",
        "  players_set = set()\n",
        "  players_list = list()\n",
        "  teams_set = set()\n",
        "\n",
        "  teams_list = list()\n",
        "  results = dict()\n",
        "  for index, (h_team, a_team, result, h_lineup, a_lineup) in df.iterrows():\n",
        "    home_players, away_players = extract_players(h_lineup, a_lineup)\n",
        "    players_set.update(home_players + away_players)\n",
        "    players_list.extend(home_players + away_players)\n",
        "    if result == 'home':\n",
        "      results.update({f'{h_team} #Wins': results.get(f'{h_team} #Wins', 0)+1})\n",
        "      results.update({f'{a_team} #Losses': results.get(f'{a_team} #Losses', 0)+1})\n",
        "    elif result == 'tie':\n",
        "      results.update({f'{h_team} #Ties': results.get(f'{h_team} #Ties', 0)+1})\n",
        "      results.update({f'{a_team} #Ties': results.get(f'{a_team} #Ties', 0)+1})\n",
        "    else:\n",
        "      results.update({f'{a_team} #Wins': results.get(f'{a_team} #Wins', 0)+1})\n",
        "      results.update({f'{h_team} #Losses': results.get(f'{h_team} #Losses', 0)+1})\n",
        "\n",
        "    teams_list.extend([h_team, a_team])\n",
        "    teams_set.update([h_team, a_team])\n",
        "    \n",
        "  if show_players:\n",
        "    for player in players_set:\n",
        "      print(f'{player} played in {players_list.count(player)} matches.')\n",
        "  if show_teams:\n",
        "    for team in teams_set:\n",
        "      print(f'{team} played {teams_list.count(team)} matches.')\n",
        "  if show_results:\n",
        "    results = od(sorted(results.items()))\n",
        "    for key, val in results.items():\n",
        "      print(f'{key}: {val}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "21iE53JDdbfE"
      },
      "outputs": [],
      "source": [
        "#@title extract_entities(df) { form-width: \"15%\" }\n",
        "def extract_entities(df: pd.DataFrame) -> typing.Tuple[set, set]:\n",
        "  players_set = set()\n",
        "  players_list = list()\n",
        "  teams_set = set()\n",
        "\n",
        "  teams_list = list()\n",
        "  # results = dict()\n",
        "  for index, (season, week, h_team, a_team, result, h_lineup, a_lineup) in df.iterrows():\n",
        "    home_players, away_players = extract_players(h_lineup, a_lineup)\n",
        "\n",
        "    players_set.update(home_players + away_players)\n",
        "    teams_set.update([h_team, a_team])\n",
        "    \n",
        "  \n",
        "  return teams_set, players_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "PFDbVt67eJOR"
      },
      "outputs": [],
      "source": [
        "#@title gen_entites(df) { form-width: \"15%\" }\n",
        "def gen_entities(df: pd.DataFrame) -> dict:\n",
        "  teams, players = extract_entities(df)\n",
        "  entities = {entity: index for index, entity in enumerate(list(players) + list(teams))}\n",
        "  return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "bHLHaaFaond8"
      },
      "outputs": [],
      "source": [
        "#@title nodes_gen(df) OK_HETERO { form-width: \"15%\" }\n",
        "\n",
        "def nodes_gen(df: pd.DataFrame) -> typing.Tuple[dict, dict]:\n",
        "  player_nodes = dict()\n",
        "  team_nodes = dict()\n",
        "  player_node_counter = 0\n",
        "  team_node_counter = 0\n",
        "\n",
        "  for index, (season, week, h_team, a_team, result, h_lineup, a_lineup) in df.iterrows():\n",
        "      home_players, away_players = extract_players(h_lineup, a_lineup)\n",
        "\n",
        "      for player_index, player in enumerate(home_players):\n",
        "        player_nodes[f'{player}@{index}'] = player_node_counter\n",
        "        player_node_counter += 1\n",
        "      for player_index, player in enumerate(away_players):\n",
        "        player_nodes[f'{player}@{index}'] = player_node_counter\n",
        "        player_node_counter += 1\n",
        "\n",
        "      team_nodes[f'{h_team}*{index}'] = team_node_counter\n",
        "      team_node_counter += 1\n",
        "\n",
        "      team_nodes[f'{a_team}*{index}'] = team_node_counter\n",
        "      team_node_counter += 1\n",
        "\n",
        "  return player_nodes, team_nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "6aAQy1k6SIwn"
      },
      "outputs": [],
      "source": [
        "#@title show_edges(df, edge, edge_type) USELESS { form-width: \"15%\" }\n",
        "def show_edges(df: pd.DataFrame, edge: torch.Tensor, edge_type: torch.Tensor, tt:str) -> NoReturn:\n",
        "  types = {\n",
        "      0: 'Won',\n",
        "      1: 'Lost To',\n",
        "      2: 'Tied With',\n",
        "      3: 'Played For',\n",
        "      4: 'Used As Player',\n",
        "      5: 'Is Before',\n",
        "      6: 'Is After'\n",
        "  }\n",
        "  t = {'p': 0, 't':1}\n",
        "  nodes = nodes_gen(df)[t[tt]]\n",
        "  r = {k:v for v, k in nodes.items()}\n",
        "  for i in range(edge_type.shape[0]):\n",
        "    head = int(edge[0][i].item())\n",
        "    tail = int(edge[1][i].item())\n",
        "    relation = int(edge_type[i].item())\n",
        "    arrow = f'=== {types[relation]} ===>'\n",
        "    print(f'{r[head]:<32}   {arrow}   {r[tail]:>32}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "pdqdirL9lxjP"
      },
      "outputs": [],
      "source": [
        "#@title home_won_gen(df) OK_HETERO { form-width: \"15%\" }\n",
        "def home_won_gen(df: pd.DataFrame, full_data_frame=None) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  home_winning_matches = df.loc[df['result'] == 'home']\n",
        "  home_winners = home_winning_matches['home_team']\n",
        "  away_losers = home_winning_matches['away_team']\n",
        "\n",
        "  winning_hashes = list()\n",
        "  losing_hashes = list()\n",
        "\n",
        "  for home, away, match in zip(home_winners, away_losers, home_winners.index):\n",
        "    winning_hashes.append(f'{home}*{match}')\n",
        "    losing_hashes.append(f'{away}*{match}')\n",
        "\n",
        "  winning_nodes = list()\n",
        "  losing_nodes = list()\n",
        "\n",
        "  if full_data_frame is None:\n",
        "    full_data_frame = df\n",
        "  _, team_nodes = nodes_gen(full_data_frame)\n",
        "\n",
        "  for winner, loser in zip(winning_hashes, losing_hashes):\n",
        "    winning_nodes.append(team_nodes[winner]) \n",
        "    losing_nodes.append(team_nodes[loser])\n",
        "\n",
        "  won_edges = torch.tensor(\n",
        "      [\n",
        "      winning_nodes,\n",
        "      losing_nodes\n",
        "      ], \n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  lost_edges = torch.tensor(\n",
        "      [\n",
        "      losing_nodes,\n",
        "      winning_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  return won_edges, lost_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "id": "LypA1Sinw94O"
      },
      "outputs": [],
      "source": [
        "#@title away_won_gen(df) OK_HETERO { form-width: \"15%\" }\n",
        "def away_won_gen(df: pd.DataFrame, full_data_frame=None) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  away_winning_matches = df.loc[df['result'] == 'away']\n",
        "  away_winners = away_winning_matches['away_team']\n",
        "  home_losers = away_winning_matches['home_team']\n",
        "\n",
        "  winning_hashes = list()\n",
        "  losing_hashes = list()\n",
        "\n",
        "  for home, away, match in zip(home_losers, away_winners, away_winners.index):\n",
        "    winning_hashes.append(f'{away}*{match}')\n",
        "    losing_hashes.append(f'{home}*{match}')\n",
        "\n",
        "  winning_nodes = list()\n",
        "  losing_nodes = list()\n",
        "\n",
        "  if full_data_frame is None:\n",
        "    full_data_frame = df\n",
        "  _, team_nodes = nodes_gen(full_data_frame)\n",
        "\n",
        "  for winner, loser in zip(winning_hashes, losing_hashes):\n",
        "    winning_nodes.append(team_nodes[winner]) \n",
        "    losing_nodes.append(team_nodes[loser])\n",
        "\n",
        "  won_edges = torch.tensor(\n",
        "      [\n",
        "      winning_nodes,\n",
        "      losing_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  lost_edges = torch.tensor(\n",
        "      [\n",
        "      losing_nodes,\n",
        "      winning_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "  \n",
        "  return won_edges, lost_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "id": "gQUZPKoQyEYp"
      },
      "outputs": [],
      "source": [
        "#@title tied_gen(df) OK_HETERO { form-width: \"15%\" }\n",
        "def tied_gen(df: pd.DataFrame, full_data_frame=None) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  tied_matches = df.loc[df['result'] == 'tie']\n",
        "  home_teams = tied_matches['home_team']\n",
        "  away_teams = tied_matches['away_team']\n",
        "\n",
        "  home_hashes = list()\n",
        "  away_hashes = list()\n",
        "\n",
        "  for home, away, match in zip(home_teams, away_teams, away_teams.index):\n",
        "    away_hashes.append(f'{away}*{match}')\n",
        "    home_hashes.append(f'{home}*{match}')\n",
        "\n",
        "  home_nodes = list()\n",
        "  away_nodes = list()\n",
        "\n",
        "  if full_data_frame is None:\n",
        "    full_data_frame = df\n",
        "  _, team_nodes = nodes_gen(full_data_frame)\n",
        "\n",
        "  for home, away in zip(home_hashes, away_hashes):\n",
        "    home_nodes.append(team_nodes[home]) \n",
        "    away_nodes.append(team_nodes[away])\n",
        "\n",
        "  home_tied_edges = torch.tensor(\n",
        "      [\n",
        "      home_nodes,\n",
        "      away_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  away_tied_edges = torch.tensor(\n",
        "      [\n",
        "      away_nodes,\n",
        "      home_nodes\n",
        "      ], \n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  return home_tied_edges, away_tied_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "form",
        "id": "IIGBYohP5znW"
      },
      "outputs": [],
      "source": [
        "#@title played_used_gen(df) OK_HETERO { form-width: \"15%\" }\n",
        "def played_used_gen(df: pd.DataFrame) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  team_nodes = list()\n",
        "  player_nodes = list()\n",
        "\n",
        "  p_nodes, t_nodes = nodes_gen(df)\n",
        "\n",
        "  for index, (season, week, h_team, a_team, result, h_lineup, a_lineup) in df.iterrows():\n",
        "    home_players, away_players = extract_players(h_lineup, a_lineup)\n",
        "\n",
        "    for home_player, away_player in zip(home_players, away_players):\n",
        "      player_nodes.append(p_nodes[f'{home_player}@{index}'])\n",
        "      team_nodes.append(t_nodes[f'{h_team}*{index}'])\n",
        "      player_nodes.append(p_nodes[f'{away_player}@{index}'])\n",
        "      team_nodes.append(t_nodes[f'{a_team}*{index}'])\n",
        "\n",
        "  played_in_edges = torch.tensor(\n",
        "      [\n",
        "       player_nodes,\n",
        "       team_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  used_edges = torch.tensor(\n",
        "      [\n",
        "       team_nodes,\n",
        "       player_nodes\n",
        "      ],\n",
        "      dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  ) \n",
        "\n",
        "  return played_in_edges, used_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "id": "Kily_SD775Ic"
      },
      "outputs": [],
      "source": [
        "#@title players_before_after_gen(df) OK_HETERO { form-width: \"15%\" }\n",
        "#TODO\n",
        "def players_before_after_gen(df: pd.DataFrame) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  player_match_hashes = list()\n",
        "\n",
        "  for index, (season, week, h_team, a_team, result, h_lineup, a_lineup) in df.iterrows():\n",
        "      home_players, away_players = extract_players(h_lineup, a_lineup)\n",
        "\n",
        "      for player in home_players + away_players:\n",
        "        player_match_hashes.append(f'{player}@{index}')\n",
        "\n",
        "\n",
        "\n",
        "  sorted_hashes = sorted(\n",
        "      player_match_hashes,\n",
        "      key=lambda w: (w.split('@')[0], int(w.split('@')[1]))\n",
        "  )\n",
        "\n",
        "  before_nodes = list()\n",
        "  after_nodes = list()\n",
        "\n",
        "  player_nodes, _ = nodes_gen(df)\n",
        "\n",
        "  for index, hash in enumerate(sorted_hashes):\n",
        "    player, match = hash.split('@')\n",
        "    before_node = player_nodes[hash]\n",
        "    try:\n",
        "      after_node = player_nodes[sorted_hashes[index+1]]\n",
        "      before_name = player_match_hashes[before_node].split('@')[0]\n",
        "      after_name = player_match_hashes[after_node].split('@')[0]\n",
        "      if before_name == after_name:\n",
        "        before_nodes.append(before_node)\n",
        "        after_nodes.append(after_node)\n",
        "    except:\n",
        "      pass\n",
        "  before_edges = torch.tensor(\n",
        "      [\n",
        "      before_nodes,\n",
        "      after_nodes\n",
        "      ], dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  after_edges = torch.tensor(\n",
        "      [\n",
        "      after_nodes,\n",
        "      before_nodes\n",
        "      ], dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  return before_edges, after_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "form",
        "id": "lk24VmcKFTF_"
      },
      "outputs": [],
      "source": [
        "#@title teams_before_after_gen(df) OK_HETERO { form-width: \"15%\" }\n",
        "def teams_before_after_gen(df: pd.DataFrame) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "  team_match_hashes = list()\n",
        "\n",
        "  for index, (season, week, h_team, a_team, result, h_lineup, a_lineup) in df.iterrows():\n",
        "      team_match_hashes.append(f'{h_team}*{index}')\n",
        "      team_match_hashes.append(f'{a_team}*{index}')\n",
        "\n",
        "  sorted_hashes = sorted(\n",
        "      team_match_hashes,\n",
        "      key= lambda w: (w.split('*')[0], int(w.split('*')[1]))\n",
        "  )\n",
        "\n",
        "  before_nodes = list()\n",
        "  after_nodes = list()\n",
        "\n",
        "  _, team_nodes = nodes_gen(df)\n",
        "\n",
        "  for index, hash in enumerate(sorted_hashes):\n",
        "    team, match = hash.split('*')\n",
        "    before_node = team_nodes[hash]\n",
        "    try:\n",
        "      after_node = team_nodes[sorted_hashes[index+1]]\n",
        "      before_name = team_match_hashes[before_node].split('*')[0]\n",
        "      after_name = team_match_hashes[after_node].split('*')[0]\n",
        "      if before_name == after_name:\n",
        "        before_nodes.append(before_node)\n",
        "        after_nodes.append(after_node)\n",
        "    except:\n",
        "      pass\n",
        "  before_edges = torch.tensor(\n",
        "      [\n",
        "      before_nodes,\n",
        "      after_nodes\n",
        "      ], dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  after_edges = torch.tensor(\n",
        "      [\n",
        "      after_nodes,\n",
        "      before_nodes\n",
        "      ], dtype=torch.long,\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  return before_edges, after_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "form",
        "id": "Rq0Mmxoxi6Sz"
      },
      "outputs": [],
      "source": [
        "#@title complete_graph_gen(df, for_players, for_teams) OK_HETERO { form-width: \"10%\" }\n",
        "def complete_graph_gen(df: pd.DataFrame, for_players: bool=True, for_teams: bool=True) -> dict:\n",
        "  home_won, away_lost = home_won_gen(df)\n",
        "  away_won, home_lost = away_won_gen(df)\n",
        "  home_tied, away_tied = tied_gen(df)\n",
        "  player_played, team_used = played_used_gen(df)\n",
        "\n",
        "  if for_players:\n",
        "    player_before, player_after = players_before_after_gen(df)\n",
        "  if for_teams:\n",
        "    team_before, team_after = teams_before_after_gen(df)\n",
        "  won_edge_index = torch.cat(\n",
        "      (home_won, away_won),\n",
        "      dim=1\n",
        "  )\n",
        "  lost_edge_index = torch.cat(\n",
        "      (away_lost, home_lost),\n",
        "      dim=1\n",
        "  )\n",
        "  tied_edge_index = torch.cat(\n",
        "      (home_tied, away_tied),\n",
        "      dim=1\n",
        "  )\n",
        "  edge_index = {\n",
        "      'won': won_edge_index,\n",
        "      'lost': lost_edge_index,\n",
        "      'tied': tied_edge_index,\n",
        "      'played': player_played,\n",
        "      'used': team_used,\n",
        "      'p_after':player_after,\n",
        "      'p_before': player_before,\n",
        "      't_after': team_after,\n",
        "      't_before': team_after\n",
        "  }   \n",
        "  return edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellView": "form",
        "id": "8N_vya3_PB75"
      },
      "outputs": [],
      "source": [
        "#@title supervision_graph_gen(df, for_players, for_teams, log_supervision_matches) OK_HETERO { form-width: \"10%\" }\n",
        "def supervision_graph_gen(df : pd.DataFrame, messaging: list, supervision: list, for_players: bool=True, for_teams: bool=True, log_supervision_matches: bool=False) -> typing.Tuple[torch.Tensor, torch.Tensor]:\n",
        "  if log_supervision_matches:\n",
        "    if model.mode == 'train':\n",
        "      mode = 'training'\n",
        "    elif model.mode == 'dev':\n",
        "      mode = 'validating'\n",
        "    elif model.mode == 'test':\n",
        "      mode = 'testing'\n",
        "    logging.info(\n",
        "        f'Messaging on matches ({messaging[0] + 1} -> {messaging[-1] + 1:>5}),\\ Model is {mode} on matches ({last_match+2} -> {last_match + 11})'\n",
        "    )\n",
        "\n",
        "  target_for_nodes = df\n",
        "\n",
        "  home_won, away_lost = home_won_gen(df.loc[messaging], full_data_frame=target_for_nodes)\n",
        "  away_won, home_lost = away_won_gen(df.loc[messaging], full_data_frame=target_for_nodes)\n",
        "  home_tied, away_tied = tied_gen(df.loc[messaging], full_data_frame=target_for_nodes)\n",
        "\n",
        "  player_played, team_used = played_used_gen(df)\n",
        "\n",
        "  if for_players:\n",
        "    player_before, player_after = players_before_after_gen(df)\n",
        "  if for_teams:\n",
        "    team_before, team_after = teams_before_after_gen(df)\n",
        "\n",
        "  won_edge_index = torch.cat(\n",
        "      (home_won, away_won),\n",
        "      dim=1\n",
        "  )\n",
        "  lost_edge_index = torch.cat(\n",
        "      (away_lost, home_lost),\n",
        "      dim=1\n",
        "  )\n",
        "  tied_edge_index = torch.cat(\n",
        "      (home_tied, away_tied),\n",
        "      dim=1\n",
        "  )\n",
        "  edge_index = {\n",
        "      'won': won_edge_index,\n",
        "      'lost': lost_edge_index,\n",
        "      'tied': tied_edge_index,\n",
        "      'played': player_played,\n",
        "      'used': team_used,\n",
        "      'p_after':player_after,\n",
        "      'p_before': player_before,\n",
        "      't_after': team_after,\n",
        "      't_before': team_after\n",
        "  }  \n",
        "  return edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "id": "YCWPYy3aPjhg"
      },
      "outputs": [],
      "source": [
        "#@title data_gen(df, remove_supervision_links, for_players, for_teams, print_edges, log_supervision_matches) OK_HETERO { form-width: \"10%\" }\n",
        "def data_gen(df: pd.DataFrame, messaging: list, supervision: list=None, remove_supervision_links: bool=True, for_players: bool=True, for_teams: bool=True, print_edges: bool=False, log_supervision_matches: bool=False) -> HeteroData:\n",
        "  if print_edges:\n",
        "    show_edges(df, edge_index, edge_type)\n",
        "  if remove_supervision_links:\n",
        "    edge_index = supervision_graph_gen(\n",
        "        df,\n",
        "        messaging=messaging,\n",
        "        supervision=supervision,\n",
        "        for_players=for_players,\n",
        "        for_teams=for_teams,\n",
        "        log_supervision_matches=log_supervision_matches\n",
        "    )\n",
        "    y = torch.tensor(\n",
        "        df.loc[supervision]['result'].map(home_result).values,\n",
        "        device=Globals.DEVICE.value\n",
        "    )\n",
        "\n",
        "  else:\n",
        "    if supervision is None:\n",
        "      supervision = df.index\n",
        "    if messaging is None:\n",
        "      messaging = df.index\n",
        "    edge_index = complete_graph_gen(df, for_players, for_teams)\n",
        "    y = torch.tensor(\n",
        "        df.loc[supervision]['result'].map(home_result).values,\n",
        "        device=Globals.DEVICE.value\n",
        "    )\n",
        "\n",
        "  data = HeteroData()\n",
        "  data['player'].x = torch.unique(edge_index['played'][0]).to(Globals.DEVICE.value).type(torch.int64)\n",
        "  data['team'].x = torch.unique(edge_index['used'][0]).to(Globals.DEVICE.value).type(torch.int64)\n",
        "  \n",
        "  data['team', 'won', 'team'].edge_index = edge_index['won']\n",
        "  data['team', 'lost_to', 'team'].edge_index = edge_index['lost']\n",
        "  data['team', 'tied_with', 'team'].edge_index = edge_index['tied']\n",
        "  data['player', 'played_for', 'team'].edge_index = edge_index['played']\n",
        "  data['team', 'used', 'player'].edge_index = edge_index['used']\n",
        "  data['player', 'is_before', 'player'].edge_index = edge_index['p_before']\n",
        "  data['player', 'is_after', 'player'].edge_index = edge_index['p_after']\n",
        "  data['team', 'is_before', 'team'].edge_index = edge_index['t_before']\n",
        "  data['team', 'is_after', 'team'].edge_index = edge_index['t_after']\n",
        "  data.y = y\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cellView": "form",
        "id": "T13E2VxefgaL"
      },
      "outputs": [],
      "source": [
        "#@title visualzie_graph(df, width, height, title, remove_supervision_links) { form-width: \"10%\" }\n",
        "def visualize_graph(df:pd.DataFrame, width: int=20, height: int=20, title: str=None, remove_supervision_links: bool=False) -> NoReturn:\n",
        "  import networkx as nx\n",
        "  import matplotlib.pyplot as plt\n",
        "  nodes = nodes_gen(df)\n",
        "  r = {k:v for v, k in nodes.items()}\n",
        "  d = data_gen(df, remove_supervision_links=remove_supervision_links)\n",
        "  G = to_networkx(d)\n",
        "  types = {\n",
        "        0: 'Won',\n",
        "        1: 'Lost To',\n",
        "        2: 'Tied With',\n",
        "        3: 'Played For',\n",
        "        4: 'Used As Player',\n",
        "        5: 'Is Before',\n",
        "        6: 'Is After'\n",
        "  }\n",
        "\n",
        "  type_color = {\n",
        "      0: '#00ff00', #won\n",
        "      1: '#ff0000', #lost to\n",
        "      2: '#e6d70e', #tied with\n",
        "      3: '#1338f0', #played for\n",
        "      4: '#f01373', #used as player\n",
        "      5: '#0f072e', #is before\n",
        "      6: '#d909cb' #is after\n",
        "  }\n",
        "\n",
        "  double_edge_types = {\n",
        "      0: '(Won[green] - Lost to[red])',\n",
        "      1: '(Lost to[red] - Won[green])',\n",
        "      2: '(Tied with[yellow])',\n",
        "      3: '(Played for[blue] - Used as Player[pink])',\n",
        "      4: '(Used as Player[pink] - Played for[blue])',\n",
        "      5: '(Is Before[dark blue] - Is After[purple])',\n",
        "      6: '(Is After[purple] - Is Before[dark blue])'\n",
        "  }\n",
        "\n",
        "  link_colors = dict(zip(\n",
        "        types.values(),\n",
        "        type_color.values()\n",
        "      )\n",
        "  )\n",
        "\n",
        "  node_colors = {\n",
        "      'player-color': '#8f0ba1',\n",
        "      'team-color': '#02fae1'   \n",
        "  }\n",
        "\n",
        "  all_colors = link_colors.copy()\n",
        "  all_colors.update(node_colors)\n",
        "\n",
        "  \n",
        "\n",
        "  for color_use in all_colors.keys():\n",
        "      plt.scatter([],[], c=[all_colors[color_use]], label=f'{color_use}')\n",
        "\n",
        "  edge_colors = list()\n",
        "  edge_labels = dict()\n",
        "\n",
        "  ######################################################## NOT OPTIMIZED\n",
        "  for edge in G.edges():\n",
        "    e = torch.tensor(edge, device=Globals.DEVICE.value)\n",
        "    for index, node_node in enumerate(d.edge_index.t()):\n",
        "      if torch.equal(e, node_node):\n",
        "        edge_colors.append(type_color[d.edge_type[index].item()])\n",
        "        label = double_edge_types[d.edge_type[index].item()]\n",
        "        edge_labels.update({edge:label})\n",
        "  colors = list()\n",
        "  node_labels = dict()\n",
        "  for node in G.nodes():\n",
        "    if '@' in r[node]:\n",
        "      colors.append(all_colors['player-color'])\n",
        "      node_labels.update({node: r[node].split('@')[0]})\n",
        "    elif '*' in r[node]:\n",
        "      colors.append(all_colors['team-color'])\n",
        "      node_labels.update({node:r[node].split('*')[0]})\n",
        "  ######################################################## NOT OPTIMIZED\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(width, height)\n",
        "  pos = nx.spring_layout(G)\n",
        "  nx.draw_networkx_nodes(G, pos, node_color=colors)\n",
        "  nx.draw_networkx_labels(G, pos, labels=node_labels)\n",
        "  nx.draw_networkx_edges(G, pos, edge_color=edge_colors, connectionstyle='arc3,rad=0.05')\n",
        "  nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
        "  plt.legend()\n",
        "  plt.title(title)\n",
        "  fig.show()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "id": "9cx2_quEQdyY"
      },
      "outputs": [],
      "source": [
        "#@title batch_gen(df, entities, log_supervision_matches) OK_HETERO { form-width: \"5%\" }\n",
        "def batch_gen(df: pd.DataFrame, entities: dict, messaging: list=None, supervision: list=None, remove_supervision_links: bool=True, log_supervision_matches: bool=False) -> HeteroData:\n",
        "  graph = data_gen(\n",
        "      df,\n",
        "      messaging=messaging,\n",
        "      supervision=supervision, \n",
        "      remove_supervision_links=remove_supervision_links,\n",
        "      log_supervision_matches=log_supervision_matches\n",
        "  )\n",
        "  \n",
        "  home_teams = list()\n",
        "  away_teams = list()\n",
        "\n",
        "  p_nodes, t_nodes = nodes_gen(df)\n",
        "  nodes = {**p_nodes, **t_nodes}\n",
        "  \n",
        "  if supervision is None:\n",
        "    supervision = df.index\n",
        "\n",
        "  indices = dict()\n",
        "  for hash, index in nodes.items():\n",
        "    if '@' in hash:\n",
        "      player = hash.split('@')[0]\n",
        "      player_id = entities[player]\n",
        "      indices.update({index:player_id})\n",
        "    elif '*' in hash:\n",
        "      team = hash.split('*')[0]\n",
        "      team_id = entities[team]\n",
        "      indices.update({index: team_id})\n",
        "  for index, (season, week, h_team, a_team, result, h_lineup, a_lineup) in df.loc[supervision].iterrows():\n",
        "      home_teams.append(nodes[f'{h_team}*{index}'])\n",
        "      away_teams.append(nodes[f'{a_team}*{index}'])\n",
        "\n",
        "  features_player = torch.tensor(\n",
        "      [indices[i.item()] for i in graph['player'].x],\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "  features_team = torch.tensor(\n",
        "      [indices[i.item()] for i in graph['team'].x],\n",
        "      device=Globals.DEVICE.value\n",
        "  )\n",
        "\n",
        "  graph['player'].x = features_player\n",
        "  graph['team'].x = features_team\n",
        "  graph.home_list = home_teams\n",
        "  graph.away_list = away_teams\n",
        "  \n",
        "  return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "cellView": "form",
        "id": "lB8wip7OtAJp"
      },
      "outputs": [],
      "source": [
        "#@title train(model, dataset, optimizer, loss_fn) { form-width: \"15%\" }\n",
        "def train(model: HeteroGNN, data: HeteroData, optimizer: torch.optim, loss_fn: torch.nn.modules.loss) -> typing.Tuple[float, int, int]:\n",
        "  batch_loss = 0\n",
        "\n",
        "  model.train()\n",
        "  out = model(data)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss = loss_fn(out, data.y)\n",
        "  batch_loss = loss.item()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  prediction = out.argmax(dim=-1)\n",
        "  correct = (prediction == data.y).sum().item()\n",
        "  all = data.y.shape[0]\n",
        "\n",
        "  return batch_loss, correct, all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cellView": "form",
        "id": "_yVJnE1o1V8a"
      },
      "outputs": [],
      "source": [
        "#@title evaluate(model, dataset) { form-width: \"25px\" }\n",
        "@torch.no_grad()\n",
        "def evaluate(model: HeteroGNN, data: HeteroConv) -> typing.Tuple[int, int]:\n",
        "  model.eval()\n",
        "\n",
        "  # for child in model.children():\n",
        "  #   for ii in range(len(child)):\n",
        "  #       if type(child[ii]) == BatchNorm1d:\n",
        "  #           child[ii].track_running_stats = False\n",
        "\n",
        "  out = model(data)\n",
        "  prediction = out.argmax(dim=-1)\n",
        "  correct = (prediction == data.y).sum().item()\n",
        "  all = data.y.shape[0]\n",
        "  model.train()\n",
        "\n",
        "  return correct, all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_Cuee66-M_HL"
      },
      "outputs": [],
      "source": [
        "#@title Dataset Download { form-width: \"15%\" }\n",
        "import requests\n",
        "from os import getcwd\n",
        "\n",
        "url_epl = \"https://raw.githubusercontent.com/jokecamp/FootballData/master/EPL%202011-2019/PL_scraped_ord.csv\"\n",
        "url_fk = \"https://raw.githubusercontent.com/masoudmousavi/Sports-Analysis-with-GNNs/main/FakeData_EPL.csv?token=ARGPVT77P62L4SHH6LT2DCLBNRTYS\"\n",
        "# current_directory = getcwd()\n",
        "filename_rl = 'dataset.csv'\n",
        "filename_fk = 'data/old_FakeData_EPL.csv'\n",
        "# req_rl = requests.get(url_epl)\n",
        "# req_fk = requests.get(url_fk)\n",
        "\n",
        "dataset_filename = filename_fk\n",
        "\n",
        "# if req_rl.status_code == 200:\n",
        "#   with open(filename_rl, 'wb') as fp:\n",
        "#     fp.write(req_rl.content)\n",
        "# else:\n",
        "#   print(f'Error downloading file at {url_epl}')\n",
        "# if req_fk.status_code == 200:\n",
        "#   with open(filename_fk, 'wb') as fp:\n",
        "#     fp.write(req_fk.content)\n",
        "# else:\n",
        "#   print(f'Error downloading file at {url_fk}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0PcBerm8OOKR"
      },
      "outputs": [],
      "source": [
        "#@title Dataset Loading and Cleaning { form-width: \"15px\" }\n",
        "dataset = pd.read_csv(\n",
        "    dataset_filename,\n",
        "    encoding='latin-1',\n",
        "    usecols=['season', 'match_week', 'home_team', 'away_team', 'result', 'home_lineup', 'away_lineup']\n",
        ")\n",
        "corrupted = dataset.loc[pd.isna(dataset['away_lineup']) | pd.isna(dataset['home_lineup'])]\n",
        "dataset = dataset.drop(corrupted.index, axis=0)\n",
        "dataset = dataset.reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cellView": "form",
        "id": "o2iqtsxu3A8k"
      },
      "outputs": [],
      "source": [
        "#@title Log { form-width: \"15%\" }\n",
        "logging.basicConfig(\n",
        "    filename='model-logs.log',\n",
        "    filemode='w',\n",
        "    level=logging.INFO\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "cellView": "form",
        "id": "k3nK5OCX4nIw"
      },
      "outputs": [],
      "source": [
        "# #@title Hyperparameters File\n",
        "# hp_file = open('hyperparameters.json', 'w')\n",
        "# hyperparameters = {\n",
        "#     \"learning_rate\": 1e-3,\n",
        "#     \"num_epochs\": 200,\n",
        "#     \"fc_dropout\":0.01,\n",
        "#     \"conv_dropout\": 0.01,\n",
        "#     \"emb_dropout\": 0.01,\n",
        "#     \"train_messaging_graph_size\": 440,\n",
        "#     \"val_messaging_graph_size\": 440,\n",
        "#     \"test_messaging_graph_size\": 440,\n",
        "#     \"iter_size\": 10,\n",
        "#     \"val_week_denom\": 50,\n",
        "#     \"test_week_denom\": 60,\n",
        "#     \"embedding_dim\": 32,\n",
        "#     \"conv_dims\":[\n",
        "#           32,\n",
        "#           32, \n",
        "#           32,\n",
        "#           32\n",
        "#     ],\n",
        "#     \"fully_connected_dims\":[\n",
        "#               32,\n",
        "#               32\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "# json.dump(hyperparameters, hp_file, indent= 4)\n",
        "# hp_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbAnO5WibS9g",
        "outputId": "a663674e-d6b2-44f0-badb-6dcf48c88b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HeteroGNN(\n",
            "  (embed): Embedding(488, 32)\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): HeteroConv(num_relations=9)\n",
            "    (1): HeteroConv(num_relations=9)\n",
            "    (2): HeteroConv(num_relations=9)\n",
            "    (3): HeteroConv(num_relations=9)\n",
            "  )\n",
            "  (fully_connected_layers): ModuleList(\n",
            "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
            "  )\n",
            "  (classifier): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#@title Model and Model Hyperparameters { form-width: \"15%\" }\n",
        "log_supervision_matches = True\n",
        "with open('hyperparameters.json', 'r') as hp_file:\n",
        "  hyperparameters = json.load(hp_file)\n",
        "learning_rate = hyperparameters[\"learning_rate\"]\n",
        "num_epochs = hyperparameters[\"num_epochs\"]\n",
        "fc_dropout = hyperparameters[\"fc_dropout\"]\n",
        "conv_dropout = hyperparameters[\"conv_dropout\"]\n",
        "emb_dropout = hyperparameters[\"emb_dropout\"]\n",
        "\n",
        "remove_supervision_links = True\n",
        "\n",
        "entities = gen_entities(dataset)\n",
        "\n",
        "######################################## Scheme 4\n",
        "train_messaging_graph_size = hyperparameters[\"train_messaging_graph_size\"]\n",
        "val_messaging_graph_size = hyperparameters[\"val_messaging_graph_size\"]\n",
        "test_messaging_graph_size = hyperparameters[\"test_messaging_graph_size\"]\n",
        "iter_size = hyperparameters[\"iter_size\"]\n",
        "val_week_denom = hyperparameters[\"val_week_denom\"]\n",
        "test_week_denom = hyperparameters[\"test_week_denom\"]\n",
        "######################################## Parameters\n",
        "\n",
        "model = HeteroGNN(\n",
        "    embedding_dims=(\n",
        "        max(entities.values()) + 1,\n",
        "        hyperparameters[\"embedding_dim\"]\n",
        "    ),\n",
        "    conv_dims=hyperparameters[\"conv_dims\"],\n",
        "    fully_connected_dims=hyperparameters[\"fully_connected_dims\"],\n",
        "    dropout={\n",
        "        \"emb\": emb_dropout,\n",
        "        \"conv\": conv_dropout,\n",
        "        \"fc\": fc_dropout\n",
        "    }\n",
        ").to(Globals.DEVICE.value)\n",
        "\n",
        "print(model)\n",
        "\n",
        "optimizer = Adam(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate\n",
        ")\n",
        "criterion = NLLLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8mb39S2BEh2a"
      },
      "outputs": [],
      "source": [
        "#@title Data Batch Maker DO NOT RUN{ form-width: \"15%\" }\n",
        "train_batches = list()\n",
        "val_batches = list()\n",
        "test_batches = list()\n",
        "\n",
        "for i in range(train_messaging_graph_size, dataset.shape[0], iter_size):\n",
        "      if i % val_week_denom == 0:\n",
        "        ######################## Validation ########################\n",
        "        from_match = i - val_messaging_graph_size\n",
        "        to_match = i - 1\n",
        "        model.mode = 'dev'\n",
        "\n",
        "        validation_df = dataset.loc[from_match: to_match, :]\n",
        "        val_graph_data = batch_gen(\n",
        "              validation_df,\n",
        "              entities=entities,\n",
        "              messaging=validation_df.index,\n",
        "              remove_supervision_links=remove_supervision_links,\n",
        "              log_supervision_matches=log_supervision_matches\n",
        "          )\n",
        "        val_batches.append(val_graph_data)\n",
        "\n",
        "      elif i % test_week_denom == 0:\n",
        "        ######################## Test ########################\n",
        "        model.eval()\n",
        "        model.mode = 'test'\n",
        "        \n",
        "        from_match = i - test_messaging_graph_size\n",
        "        to_match = i - 1\n",
        "\n",
        "        test_df = dataset.loc[from_match: to_match, :]\n",
        "        test_graph_data = batch_gen(\n",
        "            test_df,\n",
        "            entities=entities,\n",
        "            messaging=test_df.index,\n",
        "            remove_supervision_links=remove_supervision_links,\n",
        "            log_supervision_matches=log_supervision_matches\n",
        "        )\n",
        "        \n",
        "        test_batches.append(test_graph_data)\n",
        "\n",
        "      else:\n",
        "        ######################## Train ########################\n",
        "\n",
        "        from_match = i - train_messaging_graph_size\n",
        "        to_match = i - 1\n",
        "        model.mode = 'train'\n",
        "\n",
        "        train_df = dataset.loc[from_match: to_match, :]\n",
        "        train_graph_data = batch_gen(\n",
        "            train_df,\n",
        "            entities=entities,\n",
        "            messaging=train_df.index,\n",
        "            remove_supervision_links=remove_supervision_links,\n",
        "            log_supervision_matches=log_supervision_matches\n",
        "        )\n",
        "\n",
        "        train_batches.append(train_graph_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4cRCBDz5FJVj"
      },
      "outputs": [],
      "source": [
        "#@title Model Fitting Moving Partial Graph DO NOT RUN { form-width: \"15%\" }\n",
        "try:\n",
        "  train_losses = list()\n",
        "  train_accuracies = list()\n",
        "  val_accuracies = list()\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    val_correct = 0\n",
        "    val_all = 0\n",
        "    train_all = 0\n",
        "    train_correct = 0\n",
        "\n",
        "    for index, train_graph_data in enumerate(train_batches):\n",
        "       ######################## Train ########################\n",
        "        model.train()\n",
        "        model.mode = 'train'\n",
        "\n",
        "        train_batch_loss, train_batch_correct, train_batch_all = train(\n",
        "              model=model,\n",
        "              data=train_graph_data,\n",
        "              optimizer=optimizer,\n",
        "              loss_fn=criterion\n",
        "          )\n",
        "\n",
        "        print(f'Batch {index + 1} of Epoch {epoch}: Accuracy: {train_batch_correct / train_batch_all:.4f}')\n",
        "\n",
        "        epoch_loss += train_batch_loss\n",
        "        train_correct += train_batch_correct\n",
        "        train_all += train_batch_all\n",
        "\n",
        "        ######################## Validation ########################\n",
        "        model.eval()\n",
        "        model.mode = 'dev'\n",
        "\n",
        "        val_batch_correct, val_batch_all = evaluate(\n",
        "            model=model,\n",
        "            data=val_batches[index%len(val_batches)]\n",
        "        )\n",
        "\n",
        "        val_correct += val_batch_correct\n",
        "        val_all += val_batch_all\n",
        "      \n",
        "    ########## end of epoch ###########\n",
        "    print(f'{\"=\"*32} Epoch {epoch + 1} {\"=\"*32}')\n",
        "    print(f'Train Loss:          {epoch_loss:.4f}')\n",
        "    print(f'Train Cost:          {epoch_loss / train_all:.4f}')\n",
        "    print(f'Train Accuracy:      {train_correct * 100 / train_all:.3f}%')\n",
        "    print(f'Validation Accuracy: {val_correct * 100 / val_all:.3f}%')\n",
        "    logging.info(f'{\"=\"*32} Epoch {epoch + 1} {\"=\"*32}')\n",
        "    logging.info(f'Train Loss:          {epoch_loss:.4f}')\n",
        "    logging.info(f'Train Cost:          {epoch_loss / train_all:.4f}')\n",
        "    logging.info(f'Train Accuracy:      {train_correct * 100 / train_all:.3f}%')\n",
        "    logging.info(f'Validation Accuracy: {val_correct * 100 / val_all:.3f}%')\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(train_correct * 100 / train_all)\n",
        "    val_accuracies.append(val_correct * 100 / val_all)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AqnGTPwygA5O"
      },
      "outputs": [],
      "source": [
        "#@title Results DO NOT RUN{ form-width: \"15%\" }\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "t = [i for i in list(range(len(train_losses)))]\n",
        "t = np.array(t)\n",
        "y1 = np.array(train_losses)\n",
        "y2 = np.array(train_accuracies)\n",
        "y3 = np.array(val_accuracies)\n",
        "fig = plt.gcf()\n",
        "plt.plot(t, y1)\n",
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "#plt.legend(['Train', 'Validation'])\n",
        "fig.set_size_inches(20, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tnPbCyM_R3mE"
      },
      "outputs": [],
      "source": [
        "#@title Model Test DO NOT RUN{ form-width: \"25%\" }\n",
        "\n",
        "test_correct = 0\n",
        "test_all = 0\n",
        "\n",
        "for test_graph_data in test_batches:\n",
        "  model.eval()\n",
        "  model.mode = 'test'\n",
        "\n",
        "  test_batch_correct, test_batch_all = evaluate(\n",
        "      model=model,\n",
        "      data=test_graph_data\n",
        "  )\n",
        "  test_correct += test_batch_correct\n",
        "  test_all += test_batch_all\n",
        "\n",
        "print(f'Test Accuracy: {test_correct * 100 / test_all:.3f}%')\n",
        "logging.info('=' * 70)\n",
        "logging.info(f'Test Accuracy: {test_correct * 100 / test_all:.3f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CawLz0bIJkCD"
      },
      "outputs": [],
      "source": [
        "# @title Model Save DO NOT RUN\n",
        "torch.save(model.state_dict(), 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "cellView": "form",
        "id": "XTRZlmyTCWli"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"won\" edge_list\n",
            "tensor([[ 6,  7],\n",
            "        [10, 11],\n",
            "        [20, 21],\n",
            "        [ 1,  0],\n",
            "        [ 3,  2],\n",
            "        [13, 12],\n",
            "        [19, 18]], device='cuda:0')\n",
            "################################################################\n",
            "Team nodes and hashes\n",
            "0                                       Queens Park Rangers*0\n",
            "1                                       Swansea City*0\n",
            "2                                       Bolton Wanderers*1\n",
            "3                                       Arsenal*1\n",
            "4                                       Tottenham Hotspur*2\n",
            "5                                       Fulham*2\n",
            "6                                       Norwich City*3\n",
            "7                                       West Bromwich Albion*3\n",
            "8                                       Liverpool*4\n",
            "9                                       Stoke City*4\n",
            "10                                      Manchester United*5\n",
            "11                                      Sunderland*5\n",
            "12                                      Wolverhampton Wanderers*6\n",
            "13                                      Aston Villa*6\n",
            "14                                      Chelsea*7\n",
            "15                                      Newcastle United*7\n",
            "16                                      Everton*8\n",
            "17                                      Manchester City*8\n",
            "18                                      Blackburn Rovers*9\n",
            "19                                      Wigan Athletic*9\n",
            "20                                      Arsenal*10\n",
            "21                                      Queens Park Rangers*10\n",
            "################################################################\n",
            "Result of supervision matches\n",
            "tensor([1, 1, 0, 1], device='cuda:0')\n",
            "################################################################\n",
            "home teams\n",
            "[4, 8, 14, 16]\n",
            "################################################################\n",
            "away teams\n",
            "[5, 9, 15, 17]\n",
            "################################################################\n",
            "home nodes\n",
            "['Tottenham Hotspur*2', 'Liverpool*4', 'Chelsea*7', 'Everton*8']\n",
            "################################################################\n",
            "away nodes\n",
            "['Fulham*2', 'Stoke City*4', 'Newcastle United*7', 'Manchester City*8']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>season</th>\n",
              "      <th>match_week</th>\n",
              "      <th>home_team</th>\n",
              "      <th>away_team</th>\n",
              "      <th>result</th>\n",
              "      <th>home_lineup</th>\n",
              "      <th>away_lineup</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>Queens Park Rangers</td>\n",
              "      <td>Swansea City</td>\n",
              "      <td>away</td>\n",
              "      <td>Diniyar Bilyaletdinov - Ronald Zubar - Rafael ...</td>\n",
              "      <td>Ali Al Habsi - Aaron Ramsey - Grant Holt - Eri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>Bolton Wanderers</td>\n",
              "      <td>Arsenal</td>\n",
              "      <td>away</td>\n",
              "      <td>Ramires - David N'Gog - Steven Taylor - Sam Ri...</td>\n",
              "      <td>Brad Guzan - Theo Walcott - Phil Neville - Cra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>Tottenham Hotspur</td>\n",
              "      <td>Fulham</td>\n",
              "      <td>away</td>\n",
              "      <td>Ricardo Fuller - Titus Bramble - Gal Clichy ...</td>\n",
              "      <td>Peter Crouch - John Terry - Kevin Foley - Stew...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>Norwich City</td>\n",
              "      <td>West Bromwich Albion</td>\n",
              "      <td>home</td>\n",
              "      <td>Park Ji-Sung - Pavel Pogrebnyak - Magaye Gueye...</td>\n",
              "      <td>Jack Rodwell - Lucas Leiva - David Stockdale -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>Liverpool</td>\n",
              "      <td>Stoke City</td>\n",
              "      <td>away</td>\n",
              "      <td>Stefan Savic - David Jones - Luka Modric - Luk...</td>\n",
              "      <td>Philippe Senderos - Scott Dann - Simon Vukcevi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>Manchester United</td>\n",
              "      <td>Sunderland</td>\n",
              "      <td>home</td>\n",
              "      <td>Sam Hutchinson - Frank Lampard - Jay Bothroyd ...</td>\n",
              "      <td>David Goodwillie - Ross Barkley - Chris Smalli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>Wolverhampton Wanderers</td>\n",
              "      <td>Aston Villa</td>\n",
              "      <td>away</td>\n",
              "      <td>Ryan Giggs - Danny Guthrie - Fabrice Muamba - ...</td>\n",
              "      <td>Shaun Wright-Phillips - Gabriel Obertan - Sylv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>Chelsea</td>\n",
              "      <td>Newcastle United</td>\n",
              "      <td>home</td>\n",
              "      <td>Brett Emerton - Fitz Hall - Elliott Bennett - ...</td>\n",
              "      <td>Younes Kaboul - Tuncay Sanli - Christopher Sam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>Everton</td>\n",
              "      <td>Manchester City</td>\n",
              "      <td>away</td>\n",
              "      <td>Danny Simpson - Fraizer Campbell - Jake Liverm...</td>\n",
              "      <td>Declan Rudd - Leighton Baines - David Junior H...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1990</td>\n",
              "      <td>1</td>\n",
              "      <td>Blackburn Rovers</td>\n",
              "      <td>Wigan Athletic</td>\n",
              "      <td>away</td>\n",
              "      <td>Mikel Arteta - Matt Upson - Jamie Mackie - Ben...</td>\n",
              "      <td>David Silva - Wayne Hennessey - Wes Brown - Da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1990</td>\n",
              "      <td>2</td>\n",
              "      <td>Arsenal</td>\n",
              "      <td>Queens Park Rangers</td>\n",
              "      <td>home</td>\n",
              "      <td>Samir Nasri - Stephen Kelly - Steven Caulker -...</td>\n",
              "      <td>James Collins - Alex - Rory Delap - Joleon Les...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    season  match_week                home_team             away_team result  \\\n",
              "0     1990           1      Queens Park Rangers          Swansea City   away   \n",
              "1     1990           1         Bolton Wanderers               Arsenal   away   \n",
              "2     1990           1        Tottenham Hotspur                Fulham   away   \n",
              "3     1990           1             Norwich City  West Bromwich Albion   home   \n",
              "4     1990           1                Liverpool            Stoke City   away   \n",
              "5     1990           1        Manchester United            Sunderland   home   \n",
              "6     1990           1  Wolverhampton Wanderers           Aston Villa   away   \n",
              "7     1990           1                  Chelsea      Newcastle United   home   \n",
              "8     1990           1                  Everton       Manchester City   away   \n",
              "9     1990           1         Blackburn Rovers        Wigan Athletic   away   \n",
              "10    1990           2                  Arsenal   Queens Park Rangers   home   \n",
              "\n",
              "                                          home_lineup  \\\n",
              "0   Diniyar Bilyaletdinov - Ronald Zubar - Rafael ...   \n",
              "1   Ramires - David N'Gog - Steven Taylor - Sam Ri...   \n",
              "2   Ricardo Fuller - Titus Bramble - Gal Clichy ...   \n",
              "3   Park Ji-Sung - Pavel Pogrebnyak - Magaye Gueye...   \n",
              "4   Stefan Savic - David Jones - Luka Modric - Luk...   \n",
              "5   Sam Hutchinson - Frank Lampard - Jay Bothroyd ...   \n",
              "6   Ryan Giggs - Danny Guthrie - Fabrice Muamba - ...   \n",
              "7   Brett Emerton - Fitz Hall - Elliott Bennett - ...   \n",
              "8   Danny Simpson - Fraizer Campbell - Jake Liverm...   \n",
              "9   Mikel Arteta - Matt Upson - Jamie Mackie - Ben...   \n",
              "10  Samir Nasri - Stephen Kelly - Steven Caulker -...   \n",
              "\n",
              "                                          away_lineup  \n",
              "0   Ali Al Habsi - Aaron Ramsey - Grant Holt - Eri...  \n",
              "1   Brad Guzan - Theo Walcott - Phil Neville - Cra...  \n",
              "2   Peter Crouch - John Terry - Kevin Foley - Stew...  \n",
              "3   Jack Rodwell - Lucas Leiva - David Stockdale -...  \n",
              "4   Philippe Senderos - Scott Dann - Simon Vukcevi...  \n",
              "5   David Goodwillie - Ross Barkley - Chris Smalli...  \n",
              "6   Shaun Wright-Phillips - Gabriel Obertan - Sylv...  \n",
              "7   Younes Kaboul - Tuncay Sanli - Christopher Sam...  \n",
              "8   Declan Rudd - Leighton Baines - David Junior H...  \n",
              "9   David Silva - Wayne Hennessey - Wes Brown - Da...  \n",
              "10  James Collins - Alex - Rory Delap - Joleon Les...  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title VERY IMPORTANT MAIN { form-width: \"15%\" }\n",
        "\n",
        "# this is the dataframe from which you want to create your graph\n",
        "dss = dataset.loc[0: 10]\n",
        "\n",
        "# messaging which should be an iterable of indices contains matches that are in the message-passing graph\n",
        "messaging = [0, 1, 3, 5, 6, 9, 10]\n",
        "\n",
        "# supervision which should be an iterable of indices contains matches that are removed\n",
        "# from the message-passing graph and are used for Link Prediction\n",
        "# could also be None if remove_supervision_links is False\n",
        "supervision=[2, 4, 7, 8] \n",
        "\n",
        "# hd is the HeteroData graph \n",
        "hd = batch_gen(\n",
        "    dss,\n",
        "    entities=entities, # must never be anything else\n",
        "    messaging = messaging, \n",
        "    supervision=supervision, \n",
        "    remove_supervision_links=True\n",
        ")\n",
        "\n",
        "hd_ei = hd.edge_index_dict\n",
        "print('\"won\" edge_list')\n",
        "print(hd_ei[('team', 'won', 'team')].t())\n",
        "print('#' * 64)\n",
        "player_nodes, team_nodes = nodes_gen(dss)\n",
        "player_hashes = {value: key for key, value in player_nodes.items()}\n",
        "team_hashes = {value: key for key, value in team_nodes.items()}\n",
        "\n",
        "print('Team nodes and hashes')\n",
        "for i, j in team_nodes.items():\n",
        "  print(f'{j:<40}{i}')\n",
        "print('#' * 64)\n",
        "print('Result of supervision matches')\n",
        "print(hd.y)\n",
        "print('#' * 64)\n",
        "print('home teams')\n",
        "print(hd.home_list)\n",
        "print('#' * 64)\n",
        "print('away teams')\n",
        "print(hd.away_list)\n",
        "print('#' * 64)\n",
        "print('home nodes')\n",
        "print([team_hashes[team_node] for team_node in hd.home_list])\n",
        "print('#' * 64)\n",
        "print('away nodes')\n",
        "print([team_hashes[team_node] for team_node in hd.away_list])\n",
        "dss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.3599, -1.4278, -2.7743],\n",
              "        [-0.9239, -0.5582, -3.4802],\n",
              "        [-0.4334, -1.2613, -2.6826],\n",
              "        [-0.5614, -1.0585, -2.4934]], device='cuda:0',\n",
              "       grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(hd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "weeks = []\n",
        "\n",
        "for season, seasondf in dataset.groupby('season'):\n",
        "    for w, weekdf in seasondf.groupby('match_week'):\n",
        "        weeks.append(weekdf)\n",
        "        if len(weeks) > 100: break\n",
        "    if len(weeks) > 100: break\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>season</th>\n",
              "      <th>match_week</th>\n",
              "      <th>home_team</th>\n",
              "      <th>away_team</th>\n",
              "      <th>result</th>\n",
              "      <th>home_lineup</th>\n",
              "      <th>away_lineup</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>1990</td>\n",
              "      <td>11</td>\n",
              "      <td>Queens Park Rangers</td>\n",
              "      <td>Blackburn Rovers</td>\n",
              "      <td>home</td>\n",
              "      <td>Craig Gardner - Ahmed El Mohamady - Adam Hammi...</td>\n",
              "      <td>Yaya Tour - Ryan Taylor - Gabriel Tamas - Ci...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>1990</td>\n",
              "      <td>11</td>\n",
              "      <td>Wigan Athletic</td>\n",
              "      <td>Everton</td>\n",
              "      <td>tie</td>\n",
              "      <td>Jason Roberts - Rafael - Wes Brown - Anton Fer...</td>\n",
              "      <td>Bryan Ruiz - Fraizer Campbell - Yohan Cabaye -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>1990</td>\n",
              "      <td>11</td>\n",
              "      <td>Manchester City</td>\n",
              "      <td>Chelsea</td>\n",
              "      <td>home</td>\n",
              "      <td>Adam Drury - Dimitar Berbatov - Leighton Baine...</td>\n",
              "      <td>Maynor Figueroa - Fitz Hall - Joe Allen - Elli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>1990</td>\n",
              "      <td>11</td>\n",
              "      <td>Newcastle United</td>\n",
              "      <td>Wolverhampton Wanderers</td>\n",
              "      <td>tie</td>\n",
              "      <td>Mchel Salgado - Christopher Samba - Cheick T...</td>\n",
              "      <td>Kevin Doyle - Vincent Kompany - Danny Guthrie ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>1990</td>\n",
              "      <td>11</td>\n",
              "      <td>Aston Villa</td>\n",
              "      <td>Manchester United</td>\n",
              "      <td>home</td>\n",
              "      <td>Peter Odemwingie - Steven Nzonzi - John Ruddy ...</td>\n",
              "      <td>Danny Higginbotham - Sam Hutchinson - Patrice ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>1990</td>\n",
              "      <td>11</td>\n",
              "      <td>Sunderland</td>\n",
              "      <td>Liverpool</td>\n",
              "      <td>away</td>\n",
              "      <td>Danny Welbeck - Glenn Whelan - Andr Santos -...</td>\n",
              "      <td>Michael Dawson - Luke Young - Martin Petrov - ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>1990</td>\n",
              "      <td>11</td>\n",
              "      <td>Stoke City</td>\n",
              "      <td>Norwich City</td>\n",
              "      <td>away</td>\n",
              "      <td>Shaun Maloney - Scott Dann - Stiliyan Petrov -...</td>\n",
              "      <td>Pavel Pogrebnyak - Mario Balotelli - Phil Jagi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>1990</td>\n",
              "      <td>11</td>\n",
              "      <td>West Bromwich Albion</td>\n",
              "      <td>Tottenham Hotspur</td>\n",
              "      <td>away</td>\n",
              "      <td>Adel Taarabt - John Heitinga - Ivan Klasnic - ...</td>\n",
              "      <td>Zat Knight - Branislav Ivanovic - Steven Fletc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>1990</td>\n",
              "      <td>11</td>\n",
              "      <td>Fulham</td>\n",
              "      <td>Bolton Wanderers</td>\n",
              "      <td>tie</td>\n",
              "      <td>Zak Whitbread - Nedum Onuoha - Peter Crouch - ...</td>\n",
              "      <td>Petr Cech - Marc Albrighton - Ben Foster - Tom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>1990</td>\n",
              "      <td>11</td>\n",
              "      <td>Arsenal</td>\n",
              "      <td>Swansea City</td>\n",
              "      <td>away</td>\n",
              "      <td>Jordan Henderson - Anders Lindegaard - Kieran ...</td>\n",
              "      <td>Aaron Ramsey - Gareth Barry - Emmerson Boyce -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     season  match_week             home_team                away_team result  \\\n",
              "100    1990          11   Queens Park Rangers         Blackburn Rovers   home   \n",
              "101    1990          11        Wigan Athletic                  Everton    tie   \n",
              "102    1990          11       Manchester City                  Chelsea   home   \n",
              "103    1990          11      Newcastle United  Wolverhampton Wanderers    tie   \n",
              "104    1990          11           Aston Villa        Manchester United   home   \n",
              "105    1990          11            Sunderland                Liverpool   away   \n",
              "106    1990          11            Stoke City             Norwich City   away   \n",
              "107    1990          11  West Bromwich Albion        Tottenham Hotspur   away   \n",
              "108    1990          11                Fulham         Bolton Wanderers    tie   \n",
              "109    1990          11               Arsenal             Swansea City   away   \n",
              "\n",
              "                                           home_lineup  \\\n",
              "100  Craig Gardner - Ahmed El Mohamady - Adam Hammi...   \n",
              "101  Jason Roberts - Rafael - Wes Brown - Anton Fer...   \n",
              "102  Adam Drury - Dimitar Berbatov - Leighton Baine...   \n",
              "103  Mchel Salgado - Christopher Samba - Cheick T...   \n",
              "104  Peter Odemwingie - Steven Nzonzi - John Ruddy ...   \n",
              "105  Danny Welbeck - Glenn Whelan - Andr Santos -...   \n",
              "106  Shaun Maloney - Scott Dann - Stiliyan Petrov -...   \n",
              "107  Adel Taarabt - John Heitinga - Ivan Klasnic - ...   \n",
              "108  Zak Whitbread - Nedum Onuoha - Peter Crouch - ...   \n",
              "109  Jordan Henderson - Anders Lindegaard - Kieran ...   \n",
              "\n",
              "                                           away_lineup  \n",
              "100  Yaya Tour - Ryan Taylor - Gabriel Tamas - Ci...  \n",
              "101  Bryan Ruiz - Fraizer Campbell - Yohan Cabaye -...  \n",
              "102  Maynor Figueroa - Fitz Hall - Joe Allen - Elli...  \n",
              "103  Kevin Doyle - Vincent Kompany - Danny Guthrie ...  \n",
              "104  Danny Higginbotham - Sam Hutchinson - Patrice ...  \n",
              "105  Michael Dawson - Luke Young - Martin Petrov - ...  \n",
              "106  Pavel Pogrebnyak - Mario Balotelli - Phil Jagi...  \n",
              "107  Zat Knight - Branislav Ivanovic - Steven Fletc...  \n",
              "108  Petr Cech - Marc Albrighton - Ben Foster - Tom...  \n",
              "109  Aaron Ramsey - Gareth Barry - Emmerson Boyce -...  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weeks[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f01c5e702bd423382aa0664b18adf9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/101 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Last week is supervision and removed\n",
        "train_window_size = 30\n",
        "eval_window_size = 10\n",
        "first_batch = batch_gen(\n",
        "        dataset.loc[\n",
        "            weeks[0].index[0]: weeks[0].index[-1],\n",
        "            :\n",
        "        ],\n",
        "        entities=entities, # must never be anything else\n",
        "        messaging = [], \n",
        "        supervision= list(range(weeks[0].index[0], weeks[0].index[-1] + 1)), \n",
        "        remove_supervision_links=True\n",
        "    )\n",
        "train_graphs = []\n",
        "train_graphs.append(first_batch)\n",
        "eval_graphs = []\n",
        "for weeknumber, w in enumerate(tqdm(weeks)):\n",
        "    train_start_point = max(0, weeknumber - train_window_size + 1)\n",
        "    train_end_point = weeknumber\n",
        "    #print('train', train_start_point, train_end_point)\n",
        "    #Call Train On weeks[startpoint: endpoint + 1]\n",
        "    if train_end_point > 0:\n",
        "        hd = batch_gen(\n",
        "            dataset.loc[\n",
        "                weeks[0].index[0]: weeks[train_end_point].index[-1]\n",
        "            ],\n",
        "            entities=entities, # must never be anything else\n",
        "            messaging = list(range(weeks[0].index[0], weeks[train_end_point - 1].index[-1] + 1)), \n",
        "            supervision= list(range(weeks[train_end_point].index[0], weeks[train_end_point].index[-1] + 1)), \n",
        "            remove_supervision_links=True\n",
        "        )\n",
        "        train_graphs.append(hd)\n",
        "\n",
        "    \n",
        "    # print(f'train window: [{train_start_point}:{train_end_point + 1}) - window train loss: {train_window_loss: .4f} - window train accuracy: {train_window_acc*100: .2f}%')\n",
        "    # gm.addWeekMatchresulttoGraph(weeks[train_end_point], dt)\n",
        "\n",
        "\n",
        "\n",
        "    eval_start_point = train_end_point + 1\n",
        "    eval_end_point = eval_start_point + eval_window_size - 1\n",
        "    #print('eval', eval_start_point, eval_end_point)\n",
        "    if eval_end_point > len(weeks) - 1:\n",
        "        eval_end_point = len(weeks) - 1\n",
        "        #print('eval cor', eval_start_point, eval_end_point)\n",
        "        \n",
        "    if weeknumber == len(weeks) - 1:\n",
        "        continue\n",
        "\n",
        "    #if eval_start_point == 1:\n",
        "    ehd = batch_gen(\n",
        "        dataset.loc[\n",
        "            weeks[0].index[0]: weeks[eval_end_point].index[-1]\n",
        "        ],\n",
        "        entities=entities, # must never be anything else\n",
        "        messaging = list(range(weeks[0].index[0], weeks[train_end_point].index[-1] + 1)), \n",
        "        supervision= list(range(weeks[eval_start_point].index[0], weeks[eval_end_point].index[-1] + 1)), \n",
        "        remove_supervision_links=True\n",
        "    )\n",
        "    eval_graphs.append(ehd)\n",
        "    # else:\n",
        "    #     gm.addWeekMatchNodestoGraph(weeks[eval_end_point], dt)\n",
        "    #     gm.addWeekPlayerstoGraph(weeks[eval_end_point], dt)\n",
        "    # #TODO Call Eval\n",
        "    # eval_window_loss, eval_window_acc = eval_OnWindow(weeks[eval_start_point: eval_end_point + 1], gm)\n",
        "    # print(f'eval window: [{eval_start_point}:{eval_end_point}) - window eval loss: {eval_window_loss: .4f} - window eval accuracy: {eval_window_acc*100: .2f}%')\n",
        "    # #print(f'weeks[{start_point} : {end_point + 1}]')\n",
        "    # #print(f'Week: {weeknumber}, Start: {start_point}, End: {end_point}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#All weeks are supervision - nothing removed\n",
        "train_window_size = 30\n",
        "eval_window_size = 10\n",
        "first_batch = batch_gen(\n",
        "        dataset.loc[\n",
        "            weeks[0].index[0]: weeks[0].index[-1],\n",
        "            :\n",
        "        ],\n",
        "        entities=entities, # must never be anything else\n",
        "        messaging = list(range(weeks[0].index[0], weeks[0].index[-1] + 1)), \n",
        "        supervision= list(range(weeks[0].index[0], weeks[0].index[-1] + 1)), \n",
        "        remove_supervision_links=False\n",
        "    )\n",
        "train_graphs = []\n",
        "train_graphs.append(first_batch)\n",
        "eval_graphs = []\n",
        "for weeknumber, w in enumerate(tqdm(weeks)):\n",
        "    train_start_point = max(0, weeknumber - train_window_size + 1)\n",
        "    train_end_point = weeknumber\n",
        "    #print('train', train_start_point, train_end_point)\n",
        "    #Call Train On weeks[startpoint: endpoint + 1]\n",
        "    if train_end_point > 0:\n",
        "        hd = batch_gen(\n",
        "            dataset.loc[\n",
        "                weeks[0].index[0]: weeks[train_end_point].index[-1]\n",
        "            ],\n",
        "            entities=entities, # must never be anything else\n",
        "            messaging = list(range(weeks[0].index[0], weeks[train_end_point].index[-1] + 1)), \n",
        "            supervision= list(range(weeks[0].index[0], weeks[train_end_point].index[-1] + 1)), \n",
        "            remove_supervision_links=False\n",
        "        )\n",
        "        train_graphs.append(hd)\n",
        "\n",
        "    \n",
        "    # print(f'train window: [{train_start_point}:{train_end_point + 1}) - window train loss: {train_window_loss: .4f} - window train accuracy: {train_window_acc*100: .2f}%')\n",
        "    # gm.addWeekMatchresulttoGraph(weeks[train_end_point], dt)\n",
        "\n",
        "\n",
        "\n",
        "    eval_start_point = train_end_point + 1\n",
        "    eval_end_point = eval_start_point + eval_window_size - 1\n",
        "    #print('eval', eval_start_point, eval_end_point)\n",
        "    if eval_end_point > len(weeks) - 1:\n",
        "        eval_end_point = len(weeks) - 1\n",
        "        #print('eval cor', eval_start_point, eval_end_point)\n",
        "        \n",
        "    if weeknumber == len(weeks) - 1:\n",
        "        continue\n",
        "\n",
        "    #if eval_start_point == 1:\n",
        "    ehd = batch_gen(\n",
        "        dataset.loc[\n",
        "            weeks[0].index[0]: weeks[eval_end_point].index[-1]\n",
        "        ],\n",
        "        entities=entities, # must never be anything else\n",
        "        messaging = list(range(weeks[0].index[0], weeks[train_end_point].index[-1] + 1)), \n",
        "        supervision= list(range(weeks[eval_start_point].index[0], weeks[eval_end_point].index[-1] + 1)), \n",
        "        remove_supervision_links=True\n",
        "    )\n",
        "    eval_graphs.append(ehd)\n",
        "    # else:\n",
        "    #     gm.addWeekMatchNodestoGraph(weeks[eval_end_point], dt)\n",
        "    #     gm.addWeekPlayerstoGraph(weeks[eval_end_point], dt)\n",
        "    # #TODO Call Eval\n",
        "    # eval_window_loss, eval_window_acc = eval_OnWindow(weeks[eval_start_point: eval_end_point + 1], gm)\n",
        "    # print(f'eval window: [{eval_start_point}:{eval_end_point}) - window eval loss: {eval_window_loss: .4f} - window eval accuracy: {eval_window_acc*100: .2f}%')\n",
        "    # #print(f'weeks[{start_point} : {end_point + 1}]')\n",
        "    # #print(f'Week: {weeknumber}, Start: {start_point}, End: {end_point}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_graph = batch_gen(\n",
        "    dataset.loc[\n",
        "        weeks[0].index[0]: weeks[len(weeks) - 1].index[-1] + 10\n",
        "    ],\n",
        "    entities=entities, # must never be anything else\n",
        "    messaging = list(range(weeks[0].index[0],  weeks[len(weeks) - 1].index[-1] + 1)), \n",
        "    supervision= list(range(weeks[len(weeks) - 1].index[-1] + 1, weeks[len(weeks) - 1].index[-1] + 10 + 1)), \n",
        "    remove_supervision_links=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "879e0008d4714a6693cf89391f24f05f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0 - train loss 1.028381904753128 - train acc: 0.4801980198019802 - eval acc: 0.4630366492146597\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02895d21f12849eca45acc6598ed4fc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1 - train loss 0.9872966172671555 - train acc: 0.5188118811881188 - eval acc: 0.5002094240837697\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07f65c7e8f964ce7915045b4717e57cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 2 - train loss 0.9859013758083381 - train acc: 0.5118811881188119 - eval acc: 0.5101570680628272\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2179822bde0c416e91395d724d6cc70d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 3 - train loss 0.9547280049560094 - train acc: 0.5485148514851486 - eval acc: 0.533717277486911\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30c5b15de4e143ee8569a1d43a6388ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 4 - train loss 0.9171098098896517 - train acc: 0.5792079207920792 - eval acc: 0.5475392670157068\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "847a42a197c4429ba8ed461e193ff16f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 5 - train loss 0.8660227954387665 - train acc: 0.598019801980198 - eval acc: 0.5867015706806282\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80ff3ef34f5e41df9443831a63e62c5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 6 - train loss 0.8238770114903403 - train acc: 0.6475247524752475 - eval acc: 0.6108900523560209\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79e884d50c514d8ea94412f672ed0e82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 7 - train loss 0.7887769120164437 - train acc: 0.6643564356435644 - eval acc: 0.6530890052356021\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d03c5f79dd734e349c27f5fe3321b2c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 8 - train loss 0.7295259520559028 - train acc: 0.6861386138613862 - eval acc: 0.663979057591623\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67851abc186343449dc4654aefcfea99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 9 - train loss 0.6698221649863932 - train acc: 0.7178217821782178 - eval acc: 0.7007329842931938\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a80ce6faa59a42ae88fddaca50c249ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 10 - train loss 0.5994050483597387 - train acc: 0.7475247524752475 - eval acc: 0.726282722513089\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bcbc195209d4d7bbbc20e8e05b386d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 11 - train loss 0.5485693306320965 - train acc: 0.7673267326732673 - eval acc: 0.7479581151832461\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e2c4a97d3874d29a3bba1c4e3508c28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 12 - train loss 0.5073806448738174 - train acc: 0.7871287128712872 - eval acc: 0.7641884816753927\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9ea1b278ee8432ea4241f2ec99bc20e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 13 - train loss 0.4505410390620184 - train acc: 0.807920792079208 - eval acc: 0.796020942408377\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6df2277cd87745d6b8fa00859f8c4560",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 14 - train loss 0.4126871133794879 - train acc: 0.8465346534653465 - eval acc: 0.7962303664921466\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03df9478218c4812a38bf90d16aaa084",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 15 - train loss 0.43850302511807715 - train acc: 0.8287128712871287 - eval acc: 0.792041884816754\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e37f5baf35fd481d839d59eaa5964600",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 16 - train loss 0.40128934228479274 - train acc: 0.8287128712871287 - eval acc: 0.8048167539267016\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8383581d49fd43d3a74b250e3b300a11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 17 - train loss 0.33419535652098087 - train acc: 0.8663366336633663 - eval acc: 0.8339267015706806\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7b04bbf7bdf4db79a123f5d71f85301",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 18 - train loss 0.31896022917463046 - train acc: 0.8633663366336634 - eval acc: 0.8494240837696335\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef2c55c05a364552974f92cfc22fb29d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 19 - train loss 0.3480239796756518 - train acc: 0.8712871287128713 - eval acc: 0.8419895287958116\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adbd1acb9cdc4678a8d61966024d0744",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 20 - train loss 0.3026605173530481 - train acc: 0.8950495049504951 - eval acc: 0.8656544502617801\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c06b0f1bdbb4593957eeb4e7896737e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 21 - train loss 0.23600103577027226 - train acc: 0.9168316831683169 - eval acc: 0.8760209424083769\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "612a853a598b4d84832e3ad598905d3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 22 - train loss 0.21033620749286716 - train acc: 0.9198019801980198 - eval acc: 0.886806282722513\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e463a147eeb46e58d68e6010e633066",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 23 - train loss 0.24097525511895962 - train acc: 0.9118811881188119 - eval acc: 0.8879581151832461\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68cbdfcd6d524425a4eab21913b4e29a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 24 - train loss 0.24757891590015427 - train acc: 0.9148514851485149 - eval acc: 0.8735078534031414\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5b8c4090d46428db995209dd72efbfc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 25 - train loss 0.20041987890525176 - train acc: 0.9267326732673268 - eval acc: 0.8944502617801047\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5388f66d737b4da1a17d594f433a2030",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 26 - train loss 0.19624797130778138 - train acc: 0.9297029702970298 - eval acc: 0.8852356020942408\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "658b9d61272341628f64cdf8e6c2c437",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 27 - train loss 0.20453559263554025 - train acc: 0.9277227722772278 - eval acc: 0.8873298429319372\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03bf7ea80a0145ee938ebdc84d22b7d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 28 - train loss 0.16846260459780102 - train acc: 0.9306930693069307 - eval acc: 0.8958115183246074\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a0ee2ced46344dea6606f8a8fa2e2bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 29 - train loss 0.16506246460767665 - train acc: 0.9356435643564357 - eval acc: 0.9073298429319372\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(30):\n",
        "    tcorrect = 0\n",
        "    tloss = 0.0\n",
        "    ttotal = 0\n",
        "    ecorrect = 0\n",
        "    etotal = 0\n",
        "    for i in tqdm(range(len(eval_graphs)), leave= False):\n",
        "        itloss, itcorrect, ittotal = train(model, train_graphs[i], optimizer, criterion)\n",
        "        tloss += itloss\n",
        "        tcorrect += itcorrect\n",
        "        ttotal += ittotal\n",
        "\n",
        "        iecorrect, ietotal = evaluate(model, eval_graphs[i])\n",
        "        ecorrect += iecorrect\n",
        "        etotal += ietotal\n",
        "    itloss, itcorrect, ittotal = train(model, train_graphs[-1], optimizer, criterion)\n",
        "    tloss += itloss\n",
        "    tcorrect += itcorrect\n",
        "    ttotal += ittotal\n",
        "\n",
        "    print(f'epoch: {epoch} - train loss {tloss / len(train_graphs)} - train acc: {tcorrect / ttotal} - eval acc: {ecorrect/ etotal}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7, 10)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model, test_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HeteroGNN(\n",
            "  (embed): Embedding(488, 32)\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): HeteroConv(num_relations=9)\n",
            "    (1): HeteroConv(num_relations=9)\n",
            "    (2): HeteroConv(num_relations=9)\n",
            "    (3): HeteroConv(num_relations=9)\n",
            "  )\n",
            "  (fully_connected_layers): ModuleList(\n",
            "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (2): Linear(in_features=32, out_features=3, bias=True)\n",
            "  )\n",
            "  (classifier): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "log_supervision_matches = True\n",
        "with open('hyperparameters.json', 'r') as hp_file:\n",
        "  hyperparameters = json.load(hp_file)\n",
        "learning_rate = hyperparameters[\"learning_rate\"]\n",
        "num_epochs = hyperparameters[\"num_epochs\"]\n",
        "fc_dropout = hyperparameters[\"fc_dropout\"]\n",
        "conv_dropout = hyperparameters[\"conv_dropout\"]\n",
        "emb_dropout = hyperparameters[\"emb_dropout\"]\n",
        "\n",
        "remove_supervision_links = True\n",
        "\n",
        "#entities = gen_entities(dataset)\n",
        "\n",
        "######################################## Scheme 4\n",
        "train_messaging_graph_size = hyperparameters[\"train_messaging_graph_size\"]\n",
        "val_messaging_graph_size = hyperparameters[\"val_messaging_graph_size\"]\n",
        "test_messaging_graph_size = hyperparameters[\"test_messaging_graph_size\"]\n",
        "iter_size = hyperparameters[\"iter_size\"]\n",
        "val_week_denom = hyperparameters[\"val_week_denom\"]\n",
        "test_week_denom = hyperparameters[\"test_week_denom\"]\n",
        "######################################## Parameters\n",
        "\n",
        "model2 = HeteroGNN(\n",
        "    embedding_dims=(\n",
        "        max(entities.values()) + 1,\n",
        "        hyperparameters[\"embedding_dim\"]\n",
        "    ),\n",
        "    conv_dims=hyperparameters[\"conv_dims\"],\n",
        "    fully_connected_dims=hyperparameters[\"fully_connected_dims\"],\n",
        "    dropout={\n",
        "        \"emb\": emb_dropout,\n",
        "        \"conv\": conv_dropout,\n",
        "        \"fc\": fc_dropout\n",
        "    }\n",
        ").to(Globals.DEVICE.value)\n",
        "\n",
        "print(model2)\n",
        "\n",
        "optimizer2 = Adam(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate\n",
        ")\n",
        "criterion2 = NLLLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af14cda943904ec0a950ba270196e75f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "window: 0 - train loss 1.2793646812438966 - train acc: 0.32666666666666666 - eval acc: 0.29\n",
            "window: 1 - train loss 1.1261347909768422 - train acc: 0.3233333333333333 - eval acc: 0.29\n",
            "window: 2 - train loss 1.1144853154818217 - train acc: 0.3933333333333333 - eval acc: 0.28\n",
            "window: 3 - train loss 1.2660636186599732 - train acc: 0.19 - eval acc: 0.3\n",
            "window: 4 - train loss 1.1652896801630657 - train acc: 0.23 - eval acc: 0.33\n",
            "window: 5 - train loss 1.2050739725430806 - train acc: 0.27666666666666667 - eval acc: 0.33\n",
            "window: 6 - train loss 1.1748838464419047 - train acc: 0.39666666666666667 - eval acc: 0.31\n",
            "window: 7 - train loss 1.1706843733787538 - train acc: 0.38 - eval acc: 0.28\n",
            "window: 8 - train loss 1.3565096497535705 - train acc: 0.25333333333333335 - eval acc: 0.32\n",
            "window: 9 - train loss 1.3142128229141234 - train acc: 0.22333333333333333 - eval acc: 0.35\n",
            "window: 10 - train loss 1.271288847923279 - train acc: 0.2633333333333333 - eval acc: 0.35\n",
            "window: 11 - train loss 1.3257722457249959 - train acc: 0.08 - eval acc: 0.39\n",
            "window: 12 - train loss 1.197183624903361 - train acc: 0.33666666666666667 - eval acc: 0.42\n",
            "window: 13 - train loss 1.2691811680793763 - train acc: 0.36 - eval acc: 0.42\n",
            "window: 14 - train loss 1.1296263734499614 - train acc: 0.5066666666666667 - eval acc: 0.39\n",
            "window: 15 - train loss 1.2644268115361532 - train acc: 0.30666666666666664 - eval acc: 0.43\n",
            "window: 16 - train loss 1.128324180841446 - train acc: 0.37 - eval acc: 0.42\n",
            "window: 17 - train loss 1.3345605611801148 - train acc: 0.19666666666666666 - eval acc: 0.4\n",
            "window: 18 - train loss 1.1657150228818258 - train acc: 0.41333333333333333 - eval acc: 0.39\n",
            "window: 19 - train loss 0.931277581055959 - train acc: 0.6033333333333334 - eval acc: 0.31\n",
            "window: 20 - train loss 1.2684903661410014 - train acc: 0.3333333333333333 - eval acc: 0.33\n",
            "window: 21 - train loss 1.1539869566758474 - train acc: 0.44333333333333336 - eval acc: 0.29\n",
            "window: 22 - train loss 1.1599902987480164 - train acc: 0.4666666666666667 - eval acc: 0.26\n",
            "window: 23 - train loss 1.2810151775677998 - train acc: 0.31666666666666665 - eval acc: 0.26\n",
            "window: 24 - train loss 1.156439443429311 - train acc: 0.39 - eval acc: 0.24\n",
            "window: 25 - train loss 1.0578856845696767 - train acc: 0.39666666666666667 - eval acc: 0.25\n",
            "window: 26 - train loss 1.281561799844106 - train acc: 0.25333333333333335 - eval acc: 0.24\n",
            "window: 27 - train loss 1.45017116467158 - train acc: 0.11333333333333333 - eval acc: 0.25\n",
            "window: 28 - train loss 1.0808647612730662 - train acc: 0.48333333333333334 - eval acc: 0.22\n",
            "window: 29 - train loss 1.359922436873118 - train acc: 0.18333333333333332 - eval acc: 0.22\n",
            "window: 30 - train loss 1.1701314489046732 - train acc: 0.2833333333333333 - eval acc: 0.22\n",
            "window: 31 - train loss 1.3874931891759237 - train acc: 0.19333333333333333 - eval acc: 0.25\n",
            "window: 32 - train loss 1.1502632737159728 - train acc: 0.30333333333333334 - eval acc: 0.21\n",
            "window: 33 - train loss 1.2684772690137227 - train acc: 0.17333333333333334 - eval acc: 0.24\n",
            "window: 34 - train loss 1.027572105328242 - train acc: 0.44333333333333336 - eval acc: 0.2\n",
            "window: 35 - train loss 1.1866202374299368 - train acc: 0.38333333333333336 - eval acc: 0.22\n",
            "window: 36 - train loss 1.192681187391281 - train acc: 0.29333333333333333 - eval acc: 0.17\n",
            "window: 37 - train loss 1.2652018427848817 - train acc: 0.26666666666666666 - eval acc: 0.19\n",
            "window: 38 - train loss 1.363594384988149 - train acc: 0.15 - eval acc: 0.2\n",
            "window: 39 - train loss 1.2349124511082967 - train acc: 0.21333333333333335 - eval acc: 0.22\n",
            "window: 40 - train loss 1.3593805313110352 - train acc: 0.19 - eval acc: 0.26\n",
            "window: 41 - train loss 1.1395853757858276 - train acc: 0.3433333333333333 - eval acc: 0.25\n",
            "window: 42 - train loss 1.2401476581891377 - train acc: 0.23 - eval acc: 0.27\n",
            "window: 43 - train loss 1.2603262146313985 - train acc: 0.2633333333333333 - eval acc: 0.24\n",
            "window: 44 - train loss 1.2873388528823853 - train acc: 0.2 - eval acc: 0.24\n",
            "window: 45 - train loss 1.3384266455968221 - train acc: 0.18666666666666668 - eval acc: 0.25\n",
            "window: 46 - train loss 1.3714510758717855 - train acc: 0.07666666666666666 - eval acc: 0.26\n",
            "window: 47 - train loss 1.1612203617890675 - train acc: 0.31333333333333335 - eval acc: 0.25\n",
            "window: 48 - train loss 1.290040906270345 - train acc: 0.2 - eval acc: 0.24\n",
            "window: 49 - train loss 1.1628914594650268 - train acc: 0.41 - eval acc: 0.23\n",
            "window: 50 - train loss 1.1073743919531505 - train acc: 0.41 - eval acc: 0.24\n",
            "window: 51 - train loss 1.3348167379697164 - train acc: 0.13333333333333333 - eval acc: 0.27\n",
            "window: 52 - train loss 1.2088353435198467 - train acc: 0.37666666666666665 - eval acc: 0.22\n",
            "window: 53 - train loss 1.3807502746582032 - train acc: 0.09 - eval acc: 0.27\n",
            "window: 54 - train loss 1.2935152292251586 - train acc: 0.24666666666666667 - eval acc: 0.28\n",
            "window: 55 - train loss 1.3245902061462402 - train acc: 0.21 - eval acc: 0.26\n",
            "window: 56 - train loss 1.4870604117711386 - train acc: 0.11333333333333333 - eval acc: 0.31\n",
            "window: 57 - train loss 1.2410834511121114 - train acc: 0.18666666666666668 - eval acc: 0.32\n",
            "window: 58 - train loss 1.2182944337526957 - train acc: 0.31666666666666665 - eval acc: 0.32\n",
            "window: 59 - train loss 1.143949842453003 - train acc: 0.32666666666666666 - eval acc: 0.31\n",
            "window: 60 - train loss 1.2025156219800313 - train acc: 0.38333333333333336 - eval acc: 0.3\n",
            "window: 61 - train loss 1.3080077608426413 - train acc: 0.18333333333333332 - eval acc: 0.3\n",
            "window: 62 - train loss 1.262567643324534 - train acc: 0.29 - eval acc: 0.31\n",
            "window: 63 - train loss 1.1697126030921936 - train acc: 0.4633333333333333 - eval acc: 0.31\n",
            "window: 64 - train loss 1.145861236254374 - train acc: 0.3 - eval acc: 0.26\n",
            "window: 65 - train loss 1.4426913738250733 - train acc: 0.21666666666666667 - eval acc: 0.25\n",
            "window: 66 - train loss 1.2193296313285829 - train acc: 0.38333333333333336 - eval acc: 0.26\n",
            "window: 67 - train loss 1.2223441123962402 - train acc: 0.29 - eval acc: 0.26\n",
            "window: 68 - train loss 1.31048637231191 - train acc: 0.27 - eval acc: 0.25\n",
            "window: 69 - train loss 1.177041975657145 - train acc: 0.3233333333333333 - eval acc: 0.26\n",
            "window: 70 - train loss 1.351917819182078 - train acc: 0.25666666666666665 - eval acc: 0.27\n",
            "window: 71 - train loss 1.3437636057535807 - train acc: 0.25666666666666665 - eval acc: 0.27\n",
            "window: 72 - train loss 1.340002977848053 - train acc: 0.26 - eval acc: 0.29\n",
            "window: 73 - train loss 1.1658107042312622 - train acc: 0.3933333333333333 - eval acc: 0.3\n",
            "window: 74 - train loss 1.446800696849823 - train acc: 0.19333333333333333 - eval acc: 0.3\n",
            "window: 75 - train loss 1.4674981037775676 - train acc: 0.07333333333333333 - eval acc: 0.32\n",
            "window: 76 - train loss 1.2229597290356955 - train acc: 0.36333333333333334 - eval acc: 0.32\n",
            "window: 77 - train loss 1.2640090107917785 - train acc: 0.29333333333333333 - eval acc: 0.31\n",
            "window: 78 - train loss 1.4081615845362345 - train acc: 0.24666666666666667 - eval acc: 0.35\n",
            "window: 79 - train loss 1.1593685388565063 - train acc: 0.4033333333333333 - eval acc: 0.33\n",
            "window: 80 - train loss 1.2618730664253235 - train acc: 0.32 - eval acc: 0.3\n",
            "window: 81 - train loss 1.1647618949413299 - train acc: 0.31666666666666665 - eval acc: 0.3\n",
            "window: 82 - train loss 1.096864147981008 - train acc: 0.42333333333333334 - eval acc: 0.28\n",
            "window: 83 - train loss 1.2348909060160318 - train acc: 0.25333333333333335 - eval acc: 0.27\n",
            "window: 84 - train loss 1.1633458574612936 - train acc: 0.32666666666666666 - eval acc: 0.25\n",
            "window: 85 - train loss 1.4876316944758097 - train acc: 0.17333333333333334 - eval acc: 0.26\n",
            "window: 86 - train loss 1.1474108616511027 - train acc: 0.38 - eval acc: 0.24\n",
            "window: 87 - train loss 1.3281785130500794 - train acc: 0.22666666666666666 - eval acc: 0.26\n",
            "window: 88 - train loss 1.1247713764508565 - train acc: 0.49 - eval acc: 0.24\n",
            "window: 89 - train loss 1.2171218315760295 - train acc: 0.23 - eval acc: 0.23\n",
            "window: 90 - train loss 1.3400839606920878 - train acc: 0.12 - eval acc: 0.29\n",
            "window: 91 - train loss 1.2806726336479186 - train acc: 0.4166666666666667 - eval acc: 0.26666666666666666\n",
            "window: 92 - train loss 1.2021342674891153 - train acc: 0.25 - eval acc: 0.275\n",
            "window: 93 - train loss 1.2767224828402202 - train acc: 0.27666666666666667 - eval acc: 0.2714285714285714\n",
            "window: 94 - train loss 1.3110998312632243 - train acc: 0.25333333333333335 - eval acc: 0.26666666666666666\n",
            "window: 95 - train loss 1.1657612482706705 - train acc: 0.29 - eval acc: 0.28\n",
            "window: 96 - train loss 1.1697230537732441 - train acc: 0.24666666666666667 - eval acc: 0.275\n",
            "window: 97 - train loss 1.3215857148170471 - train acc: 0.18 - eval acc: 0.3\n",
            "window: 98 - train loss 1.1553673307100931 - train acc: 0.32666666666666666 - eval acc: 0.3\n",
            "window: 99 - train loss 1.3540933767954508 - train acc: 0.24333333333333335 - eval acc: 0.4\n"
          ]
        }
      ],
      "source": [
        "for i in tqdm(range(len(eval_graphs)), leave= False):\n",
        "    tcorrect = 0\n",
        "    tloss = 0.0\n",
        "    ttotal = 0\n",
        "    ecorrect = 0\n",
        "    etotal = 0\n",
        "    for epoch in range(30):\n",
        "        itloss, itcorrect, ittotal = train(model2, train_graphs[i], optimizer2, criterion2)\n",
        "        tloss += itloss\n",
        "        tcorrect += itcorrect\n",
        "        ttotal += ittotal\n",
        "\n",
        "        iecorrect, ietotal = evaluate(model2, eval_graphs[i])\n",
        "        ecorrect += iecorrect\n",
        "        etotal += ietotal\n",
        "    # itloss, itcorrect, ittotal = train(model2, train_graphs[-1], optimizer, criterion)\n",
        "    # tloss += itloss\n",
        "    # tcorrect += itcorrect\n",
        "    # ttotal += ittotal\n",
        "\n",
        "    print(f'window: {i} - train loss {tloss / 30} - train acc: {tcorrect / ttotal} - eval acc: {ecorrect/ etotal}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(32, 100)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model2, test_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['season', 'match_week', 'home_team', 'away_team', 'result',\n",
              "       'home_lineup', 'away_lineup'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dff.iloc[-1].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Diniyar Bilyaletdinov - Ronald Zubar - Rafael van der Vaart - Peter Lvenkrands - Cameron Jerome - Louis Saha - Alex - Mahamadou Diarra - James Collins - Damien Duff - Joleon Lescott - '"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weeks[0]['home_lineup'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "cellView": "form",
        "id": "fTEsrUB1bbIy"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataframe' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_16710/3083367430.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mmessaging_indcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0msupervision_indcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessaging_indcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   hetero_data = batch_gen(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataframe' is not defined"
          ]
        }
      ],
      "source": [
        "#@title Custom Train Method { form-width: \"15%\" }\n",
        "model.reset_parameters()\n",
        "\n",
        "dataframe_train = dataset.loc[0:6800, :]\n",
        "data_frame_val = dataset.loc[6801:, :]\n",
        "\n",
        "for _ in range(100):\n",
        "  messaging_indcs = dataframe.sample(frac=0.85).index\n",
        "  supervision_indcs = dataframe.drop(messaging_indcs).index\n",
        "  hetero_data = batch_gen(\n",
        "      dataframe_train, \n",
        "      entities, \n",
        "      remove_supervision_links=True,\n",
        "      messaging=messaging_indcs,\n",
        "      supervision=supervision_indcs\n",
        "  )\n",
        "\n",
        "  print(train(model, hetero_data, optimizer, criterion))\n",
        "\n",
        "messaging_indcs = dataframe_val.sample(frac=0.85).index\n",
        "supervision_indcs = dataframe_val.drop(messaging_indcs).index\n",
        "# evaluate(model, hetero_data2)\n",
        "hetero_data2 = batch_gen(\n",
        "    dataframe_val, \n",
        "    entities, \n",
        "    remove_supervision_links=True,\n",
        "    supervision=supervision_indcs\n",
        ")\n",
        "for _ in range(15):\n",
        "  print(train(model, hetero_data, optimizer, criterion))\n",
        "\n",
        "\n",
        "for _ in range(150):\n",
        "  print(train(model, hetero_data2, optimizer, criterion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zDZDgbsEAusX"
      },
      "outputs": [],
      "source": [
        "#@title Custom Training Method 2  { form-width: \"15%\" }\n",
        "\n",
        "ds = dataset.loc[:6549, :]\n",
        "\n",
        "hd = batch_gen(\n",
        "    ds,\n",
        "    entities=entities,\n",
        "    messaging=ds.index,\n",
        "    supervision=ds.sample(frac=0.3).index,\n",
        "    remove_supervision_links=False)\n",
        "for i in range(10000):\n",
        "  for j in range(300, 3200, 100):\n",
        "\n",
        "    ds = dataset.loc[j - 300:j - 1, :]\n",
        "\n",
        "    r = ds.sample(frac=0.1).index\n",
        "\n",
        "    hd = batch_gen(\n",
        "        ds,\n",
        "        entities=entities,\n",
        "        messaging=ds.index,\n",
        "        supervision=r,\n",
        "        remove_supervision_links=False\n",
        "    )\n",
        "    \n",
        "    print(f'Epoch {i+1}: {train(model, hd, optimizer, criterion)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "cellView": "form",
        "id": "6gpt1PVxZf4W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.14763778448104858, 4, 5)\n",
            "(5, 5)\n"
          ]
        }
      ],
      "source": [
        "#@title Custom Train Method 3 { form-width: \"15%\" }\n",
        "df = dataset.loc[:9, :]\n",
        "\n",
        "h_data = batch_gen(\n",
        "    df,\n",
        "    entities=entities,\n",
        "    supervision=[2, 3, 4, 9, 6],\n",
        "    remove_supervision_links=False\n",
        ")\n",
        "\n",
        "print(train(model, h_data, optimizer, criterion))\n",
        "print(evaluate(model, h_data))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SP Conference-Hetero.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a0b06899928c76d5fd0b72a60332a42f663199fed6ba4139629e084b70e9c24d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
